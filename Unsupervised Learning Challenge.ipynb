{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Author From Text\n",
    "\n",
    "I want to make a model that can predict who has written a story based on its text features.  To accomplish this, I will use data from the NLTK 'Gutenberg' corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import genesis\n",
    "from nltk.corpus import webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning / processing / language parsing\n",
    "\n",
    "I need to make sure my data is clean and workable when I make my features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean the data.\n",
    "persuasion_raw = gutenberg.raw('austen-persuasion.txt')\n",
    "alice_raw = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "alice = re.sub(r'CHAPTER [A-Z].','', alice_raw)\n",
    "alice = text_cleaner(alice)\n",
    "\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion_raw)\n",
    "persuasion = text_cleaner(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a sample of each text, to prevent memory errors\n",
    "alicetr = alice[:30000]\n",
    "persuasiontr = persuasion[:30000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Features\n",
    "\n",
    "I must extract features from the text, through a process called NLP, Natural Language Processing. Though there are many ways to accomplish this, I will use 2: Bag of Words and Term-Frequency with Inverse Document Frequency, or TF IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "I will use the bag of words technique first. To use this, I will need to process the texts down to sentences. From there, I will extract information on each sentences verbosity and punctuation use. I will use this information to characterize each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_doc = nlp(alicetr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "persuasion_doc = nlp(persuasiontr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Down, the, Rabbit, -, Hole, Alice, was, begin...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  (Down, the, Rabbit, -, Hole, Alice, was, begin...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                         (I, shall, be, late, !, ')  Carroll"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = pd.DataFrame(alice_sents + persuasion_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_author'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    df['punctuation'] = 0\n",
    "    df['other punctuation'] = 0\n",
    "    df.loc[:, '.'] = 0\n",
    "    df.loc[:, '?'] = 0\n",
    "    df.loc[:, '!'] = 0\n",
    "    df.loc[:, ','] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation, stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # Get number of punctuation in a sentence\n",
    "        puncs = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     token.is_punct\n",
    "                 )]\n",
    "        # Increase punctuation count by how many were use\n",
    "        for punc in puncs:\n",
    "            df.loc[i,'punctuation'] += 1\n",
    "            try:\n",
    "                df.loc[i,punc] += 1\n",
    "            except:\n",
    "                df.loc[i,'other punctuation'] += 1\n",
    "                \n",
    "            \n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(alicewords + persuasionwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>earth</th>\n",
       "      <th>15</th>\n",
       "      <th>tell</th>\n",
       "      <th>importance</th>\n",
       "      <th>model</th>\n",
       "      <th>noise</th>\n",
       "      <th>row</th>\n",
       "      <th>give</th>\n",
       "      <th>disrespectfully</th>\n",
       "      <th>spite</th>\n",
       "      <th>...</th>\n",
       "      <th>rabbit</th>\n",
       "      <th>totally</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_author</th>\n",
       "      <th>punctuation</th>\n",
       "      <th>other punctuation</th>\n",
       "      <th>.</th>\n",
       "      <th>?</th>\n",
       "      <th>!</th>\n",
       "      <th>,</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(Down, the, Rabbit, -, Hole, Alice, was, begin...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1637 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   earth 15  tell importance model  noise  row  give disrespectfully spite  \\\n",
       "0      0  0     0          0     0      0    0     0               0     0   \n",
       "1      0  0     0          0     0      0    0     0               0     0   \n",
       "2      0  0     0          0     0      0    0     0               0     0   \n",
       "3      0  0     0          0     0      0    0     0               0     0   \n",
       "4      0  0     0          0     0      0    0     0               0     0   \n",
       "\n",
       "  ... rabbit  totally                                      text_sentence  \\\n",
       "0 ...      1        0  (Down, the, Rabbit, -, Hole, Alice, was, begin...   \n",
       "1 ...      1        0  (So, she, was, considering, in, her, own, mind...   \n",
       "2 ...      1        0  (There, was, nothing, so, VERY, remarkable, in...   \n",
       "3 ...      0        0                                      (Oh, dear, !)   \n",
       "4 ...      0        0                         (I, shall, be, late, !, ')   \n",
       "\n",
       "   text_author punctuation  other punctuation  .  ?  !  ,  \n",
       "0      Carroll          11                  6  0  1  0  4  \n",
       "1      Carroll           7                  3  1  0  0  3  \n",
       "2      Carroll           4                  2  0  0  1  1  \n",
       "3      Carroll           1                  0  0  0  1  0  \n",
       "4      Carroll           2                  1  0  0  1  0  \n",
       "\n",
       "[5 rows x 1637 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to data frame for the BOW\n",
    "alice_sents = pd.DataFrame(alice_sents)\n",
    "# Get BOW features\n",
    "alice_word_counts = bow_features(alice_sents, common_words)\n",
    "alice_word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>earth</th>\n",
       "      <th>15</th>\n",
       "      <th>tell</th>\n",
       "      <th>importance</th>\n",
       "      <th>model</th>\n",
       "      <th>noise</th>\n",
       "      <th>row</th>\n",
       "      <th>give</th>\n",
       "      <th>disrespectfully</th>\n",
       "      <th>spite</th>\n",
       "      <th>...</th>\n",
       "      <th>rabbit</th>\n",
       "      <th>totally</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_author</th>\n",
       "      <th>punctuation</th>\n",
       "      <th>other punctuation</th>\n",
       "      <th>.</th>\n",
       "      <th>?</th>\n",
       "      <th>!</th>\n",
       "      <th>,</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Sir, Walter, Elliot, ,, of, Kellynch, Hall, ,...</td>\n",
       "      <td>Austen</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(This, was, the, page, at, which, the, favouri...</td>\n",
       "      <td>Austen</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Walter, Elliot, ,, born, March, 1, ,, 1760, ,...</td>\n",
       "      <td>Austen</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(of, South, Park, ,, in, the, county, of, Glou...</td>\n",
       "      <td>Austen</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(\", Precisely, such, had, the, paragraph, orig...</td>\n",
       "      <td>Austen</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1637 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  earth  15 tell  importance  model noise row  give  disrespectfully  spite  \\\n",
       "0     0   0    0           0      0     0   0     0                0      0   \n",
       "1     0   0    0           0      0     0   0     0                0      0   \n",
       "2     0   1    0           0      0     0   0     0                0      0   \n",
       "3     0   0    0           0      0     0   0     0                0      0   \n",
       "4     0   0    0           0      0     0   0     0                0      0   \n",
       "\n",
       "  ...  rabbit  totally                                      text_sentence  \\\n",
       "0 ...       0        0  (Sir, Walter, Elliot, ,, of, Kellynch, Hall, ,...   \n",
       "1 ...       0        0  (This, was, the, page, at, which, the, favouri...   \n",
       "2 ...       0        0  (Walter, Elliot, ,, born, March, 1, ,, 1760, ,...   \n",
       "3 ...       0        0  (of, South, Park, ,, in, the, county, of, Glou...   \n",
       "4 ...       0        0  (\", Precisely, such, had, the, paragraph, orig...   \n",
       "\n",
       "   text_author  punctuation other punctuation  .  ?  !   ,  \n",
       "0       Austen           15                 4  1  0  0  10  \n",
       "1       Austen            4                 3  1  0  0   0  \n",
       "2       Austen            9                 0  1  0  0   8  \n",
       "3       Austen           17                 6  1  0  0  10  \n",
       "4       Austen           12                 3  1  0  0   8  \n",
       "\n",
       "[5 rows x 1637 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to data frame for the BOW\n",
    "persuasion_sents = pd.DataFrame(persuasion_sents)\n",
    "# Get BOW features\n",
    "persuasion_word_counts = bow_features(persuasion_sents, common_words)\n",
    "persuasion_word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_word_counts = pd.concat([alice_word_counts,persuasion_word_counts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf idf\n",
    "\n",
    "The next technique I will use is TF IDF. Unlike the Bag of Words technique, this one is unsupervised, meaning that it extracts the feature information on its own. TF IDF works by placing weights on works based on how many times they appear in the document.  From there, meaning is ascribed to each sentence or paragraph based on the words that are used within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = gutenberg.paras('carroll-alice.txt')\n",
    "#processing\n",
    "alice_paras=[]\n",
    "for paragraph in alice:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    alice_paras.append(' '.join(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 789\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(alice_paras, test_size=0.5, random_state=42)\n",
    "\n",
    "#Applying the vectorizer\n",
    "alice_paras_tfidf=vectorizer.fit_transform(alice_paras)\n",
    "print(\"Number of features: %d\" % alice_paras_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(alice_paras_tfidf, test_size=0.5, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "X_test_tfidf_csr = X_test_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 85.65388303012395\n",
      "Component 0:\n",
      "' I didn ' t know it was YOUR table ,' said Alice ; ' it ' s laid for a great many more than three .'                                                                                                                                                                                                                                                                                                                                                 0.866940\n",
      "' Hold your tongue !'                                                                                                                                                                                                                                                                                                                                                                                                                                 0.866940\n",
      "' Well , I should like to be a LITTLE larger , sir , if you wouldn ' t mind ,' said Alice : ' three inches is such a wretched height to be .'                                                                                                                                                                                                                                                                                                         0.866940\n",
      "' I haven ' t the least idea what you ' re talking about ,' said Alice .                                                                                                                                                                                                                                                                                                                                                                              0.866940\n",
      "There seemed to be no use in waiting by the little door , so she went back to the table , half hoping she might find another key on it , or at any rate a book of rules for shutting people up like telescopes : this time she found a little bottle on it , (' which certainly was not here before ,' said Alice ,) and round the neck of the bottle was a paper label , with the words ' DRINK ME ' beautifully printed on it in large letters .    0.670578\n",
      "Oh dear , what nonsense I ' m talking !'                                                                                                                                                                                                                                                                                                                                                                                                              0.595551\n",
      "' THAT generally takes some time ,' interrupted the Gryphon .                                                                                                                                                                                                                                                                                                                                                                                         0.544818\n",
      "' This here young lady ,' said the Gryphon , ' she wants for to know your history , she do .'                                                                                                                                                                                                                                                                                                                                                         0.481352\n",
      "Alice looked down at them , and considered a little before she gave her answer .                                                                                                                                                                                                                                                                                                                                                                      0.468931\n",
      "' I ' m afraid I am , sir ,' said Alice ; ' I can ' t remember things as I used  and I don ' t keep the same size for ten minutes together !'                                                                                                                                                                                                                                                                                                         0.462957\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "* * * * * *                                                                                                                                 0.864756\n",
      "' Never mind !'                                                                                                                             0.675001\n",
      "' The twinkling of the what ?'                                                                                                              0.633534\n",
      "' We must burn the house down !'                                                                                                            0.611642\n",
      "' Perhaps it doesn ' t understand English ,' thought Alice ; ' I daresay it ' s a French mouse , come over with William the Conqueror .'    0.576151\n",
      "' What ' s in it ?'                                                                                                                         0.575378\n",
      "' Call the next witness !'                                                                                                                  0.575110\n",
      "' Well , it must be removed ,' said the King very decidedly , and he called the Queen , who was passing at the moment , ' My dear !         0.569326\n",
      "' That ' s the most important piece of evidence we ' ve heard yet ,' said the King , rubbing his hands ; ' so now let the jury '            0.530933\n",
      "' Treacle ,' said the Dormouse , without considering at all this time .                                                                     0.492746\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "' Now tell me , Pat , what ' s that in the window ?'                                                                                                                                                                                                                                    0.646228\n",
      "' Stolen !'                                                                                                                                                                                                                                                                             0.541325\n",
      "' HE might bite ,' Alice cautiously replied , not feeling at all anxious to have the experiment tried .                                                                                                                                                                                 0.364230\n",
      "[ later editions continued as follows The Panther took pie - crust , and gravy , and meat , While the Owl had the dish as its share of the treat .                                                                                                                                      0.348919\n",
      "So she swallowed one of the cakes , and was delighted to find that she began shrinking directly .                                                                                                                                                                                       0.345799\n",
      "' But what am I to do ?'                                                                                                                                                                                                                                                                0.332567\n",
      "' Then you keep moving round , I suppose ?'                                                                                                                                                                                                                                             0.320708\n",
      "' Give your evidence ,' said the King ; ' and don ' t be nervous , or I ' ll have you executed on the spot .'                                                                                                                                                                           0.317830\n",
      "There was a table set out under a tree in front of the house , and the March Hare and the Hatter were having tea at it : a Dormouse was sitting between them , fast asleep , and the other two were using it as a cushion , resting their elbows on it , and talking over its head .    0.314900\n",
      "Here the other guinea - pig cheered , and was suppressed .                                                                                                                                                                                                                              0.300906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "' Wake up , Alice dear !'                                                                                                                                                                  0.744423\n",
      "The Caterpillar was the first to speak .                                                                                                                                                   0.509376\n",
      "' It isn ' t mine ,' said the Hatter .                                                                                                                                                     0.505852\n",
      "' Hush !                                                                                                                                                                                   0.496242\n",
      "This speech caused a remarkable sensation among the party .                                                                                                                                0.484304\n",
      "' They told me you had been to her , And mentioned me to him : She gave me a good character , But said I could not swim .                                                                  0.459341\n",
      "' THAT generally takes some time ,' interrupted the Gryphon .                                                                                                                              0.440927\n",
      "CHAPTER I .                                                                                                                                                                                0.413360\n",
      "' Oh , you foolish Alice !'                                                                                                                                                                0.411554\n",
      "But here , to Alice ' s great surprise , the Duchess ' s voice died away , even in the middle of her favourite word ' moral ,' and the arm that was linked into hers began to tremble .    0.395745\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "' Oh , you foolish Alice !'                                                                                                                                                                                                                                                                                                         0.426211\n",
      "' How fond she is of finding morals in things !'                                                                                                                                                                                                                                                                                    0.390203\n",
      "' That is not said right ,' said the Caterpillar .                                                                                                                                                                                                                                                                                  0.326266\n",
      "' Hush !                                                                                                                                                                                                                                                                                                                            0.310637\n",
      "She drew her foot as far down the chimney as she could , and waited till she heard a little animal ( she couldn ' t guess of what sort it was ) scratching and scrambling about in the chimney close above her : then , saying to herself ' This is Bill ,' she gave one sharp kick , and waited to see what would happen next .    0.283008\n",
      "' But what did the Dormouse say ?'                                                                                                                                                                                                                                                                                                  0.255639\n",
      "CHAPTER X .                                                                                                                                                                                                                                                                                                                         0.255108\n",
      "' Well , I never heard it before ,' said the Mock Turtle ; ' but it sounds uncommon nonsense .'                                                                                                                                                                                                                                     0.254458\n",
      "' They were learning to draw ,' the Dormouse went on , yawning and rubbing its eyes , for it was getting very sleepy ; ' and they drew all manner of things  everything that begins with an M '                                                                                                                                     0.244629\n",
      "ALICE ' S RIGHT FOOT , ESQ .                                                                                                                                                                                                                                                                                                        0.230858\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(200)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "X_test_lsa = lsa.transform(X_test_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>text_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.390229</td>\n",
       "      <td>-0.081510</td>\n",
       "      <td>-0.092912</td>\n",
       "      <td>-0.115203</td>\n",
       "      <td>-0.001856</td>\n",
       "      <td>0.025481</td>\n",
       "      <td>-0.080522</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.033934</td>\n",
       "      <td>-0.025451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054166</td>\n",
       "      <td>-0.020628</td>\n",
       "      <td>-0.013088</td>\n",
       "      <td>-0.029342</td>\n",
       "      <td>0.033374</td>\n",
       "      <td>0.012718</td>\n",
       "      <td>-0.011593</td>\n",
       "      <td>-0.030338</td>\n",
       "      <td>0.040863</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.008750</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>-0.012520</td>\n",
       "      <td>0.029321</td>\n",
       "      <td>0.020522</td>\n",
       "      <td>-0.031095</td>\n",
       "      <td>0.007145</td>\n",
       "      <td>0.022515</td>\n",
       "      <td>0.015506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086814</td>\n",
       "      <td>0.006658</td>\n",
       "      <td>0.066839</td>\n",
       "      <td>0.006971</td>\n",
       "      <td>0.040019</td>\n",
       "      <td>-0.005285</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>-0.047362</td>\n",
       "      <td>-0.062244</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.161407</td>\n",
       "      <td>0.083050</td>\n",
       "      <td>0.174341</td>\n",
       "      <td>-0.101325</td>\n",
       "      <td>0.202759</td>\n",
       "      <td>0.189029</td>\n",
       "      <td>0.007689</td>\n",
       "      <td>0.036803</td>\n",
       "      <td>-0.047261</td>\n",
       "      <td>0.104901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005538</td>\n",
       "      <td>-0.005295</td>\n",
       "      <td>0.019985</td>\n",
       "      <td>0.079596</td>\n",
       "      <td>0.033792</td>\n",
       "      <td>0.018850</td>\n",
       "      <td>0.039365</td>\n",
       "      <td>-0.062420</td>\n",
       "      <td>-0.015330</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.035440</td>\n",
       "      <td>-0.030696</td>\n",
       "      <td>0.049149</td>\n",
       "      <td>-0.035796</td>\n",
       "      <td>-0.064699</td>\n",
       "      <td>0.007019</td>\n",
       "      <td>-0.008836</td>\n",
       "      <td>-0.082575</td>\n",
       "      <td>-0.057086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067636</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>-0.048086</td>\n",
       "      <td>0.053531</td>\n",
       "      <td>0.158641</td>\n",
       "      <td>0.042247</td>\n",
       "      <td>0.038193</td>\n",
       "      <td>0.038286</td>\n",
       "      <td>-0.055755</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.018448</td>\n",
       "      <td>-0.007904</td>\n",
       "      <td>-0.023741</td>\n",
       "      <td>0.059271</td>\n",
       "      <td>-0.102216</td>\n",
       "      <td>0.124459</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.022856</td>\n",
       "      <td>0.029614</td>\n",
       "      <td>0.011213</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018457</td>\n",
       "      <td>-0.003023</td>\n",
       "      <td>-0.005513</td>\n",
       "      <td>-0.006433</td>\n",
       "      <td>0.006509</td>\n",
       "      <td>-0.015058</td>\n",
       "      <td>-0.021465</td>\n",
       "      <td>-0.035362</td>\n",
       "      <td>0.017654</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.390229 -0.081510 -0.092912 -0.115203 -0.001856  0.025481 -0.080522   \n",
       "1  0.010002  0.008750  0.002558 -0.012520  0.029321  0.020522 -0.031095   \n",
       "2  0.161407  0.083050  0.174341 -0.101325  0.202759  0.189029  0.007689   \n",
       "3  0.000100 -0.035440 -0.030696  0.049149 -0.035796 -0.064699  0.007019   \n",
       "4  0.018448 -0.007904 -0.023741  0.059271 -0.102216  0.124459  0.000258   \n",
       "\n",
       "          7         8         9     ...            191       192       193  \\\n",
       "0 -0.000010  0.033934 -0.025451     ...       0.054166 -0.020628 -0.013088   \n",
       "1  0.007145  0.022515  0.015506     ...       0.086814  0.006658  0.066839   \n",
       "2  0.036803 -0.047261  0.104901     ...       0.005538 -0.005295  0.019985   \n",
       "3 -0.008836 -0.082575 -0.057086     ...       0.067636  0.005600 -0.048086   \n",
       "4  0.022856  0.029614  0.011213     ...      -0.018457 -0.003023 -0.005513   \n",
       "\n",
       "        194       195       196       197       198       199  text_author  \n",
       "0 -0.029342  0.033374  0.012718 -0.011593 -0.030338  0.040863      Carroll  \n",
       "1  0.006971  0.040019 -0.005285  0.002384 -0.047362 -0.062244      Carroll  \n",
       "2  0.079596  0.033792  0.018850  0.039365 -0.062420 -0.015330      Carroll  \n",
       "3  0.053531  0.158641  0.042247  0.038193  0.038286 -0.055755      Carroll  \n",
       "4 -0.006433  0.006509 -0.015058 -0.021465 -0.035362  0.017654      Carroll  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_train_lsa = pd.DataFrame(X_train_lsa)\n",
    "alice_train_lsa['text_author'] = 'Carroll'\n",
    "alice_train_lsa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>text_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011476</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>-0.010022</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.021190</td>\n",
       "      <td>-0.000270</td>\n",
       "      <td>-0.025158</td>\n",
       "      <td>-0.003606</td>\n",
       "      <td>0.016723</td>\n",
       "      <td>-0.020666</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004500</td>\n",
       "      <td>0.049011</td>\n",
       "      <td>0.009077</td>\n",
       "      <td>-0.014243</td>\n",
       "      <td>0.005189</td>\n",
       "      <td>-0.018279</td>\n",
       "      <td>0.014966</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>-0.036490</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.239580</td>\n",
       "      <td>-0.031796</td>\n",
       "      <td>-0.201286</td>\n",
       "      <td>-0.042940</td>\n",
       "      <td>0.080991</td>\n",
       "      <td>0.038739</td>\n",
       "      <td>-0.072529</td>\n",
       "      <td>0.035249</td>\n",
       "      <td>-0.012633</td>\n",
       "      <td>0.067849</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008668</td>\n",
       "      <td>0.052009</td>\n",
       "      <td>-0.086604</td>\n",
       "      <td>0.117077</td>\n",
       "      <td>-0.097697</td>\n",
       "      <td>-0.007316</td>\n",
       "      <td>0.021518</td>\n",
       "      <td>-0.009194</td>\n",
       "      <td>0.064450</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.242169</td>\n",
       "      <td>0.017704</td>\n",
       "      <td>-0.214895</td>\n",
       "      <td>-0.098039</td>\n",
       "      <td>0.149758</td>\n",
       "      <td>0.050317</td>\n",
       "      <td>-0.157665</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.090418</td>\n",
       "      <td>-0.045324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101755</td>\n",
       "      <td>-0.028113</td>\n",
       "      <td>-0.107991</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>0.022031</td>\n",
       "      <td>-0.122684</td>\n",
       "      <td>-0.081450</td>\n",
       "      <td>-0.050979</td>\n",
       "      <td>-0.089174</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.265044</td>\n",
       "      <td>0.034239</td>\n",
       "      <td>0.055453</td>\n",
       "      <td>-0.076710</td>\n",
       "      <td>0.050174</td>\n",
       "      <td>0.070990</td>\n",
       "      <td>-0.084321</td>\n",
       "      <td>0.013203</td>\n",
       "      <td>0.058421</td>\n",
       "      <td>-0.024091</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006524</td>\n",
       "      <td>-0.048066</td>\n",
       "      <td>-0.103454</td>\n",
       "      <td>0.027076</td>\n",
       "      <td>-0.022362</td>\n",
       "      <td>-0.020080</td>\n",
       "      <td>0.108345</td>\n",
       "      <td>-0.062217</td>\n",
       "      <td>-0.022074</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.011476  0.000920 -0.010022  0.001329  0.021190 -0.000270 -0.025158   \n",
       "1  0.239580 -0.031796 -0.201286 -0.042940  0.080991  0.038739 -0.072529   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.242169  0.017704 -0.214895 -0.098039  0.149758  0.050317 -0.157665   \n",
       "4  0.265044  0.034239  0.055453 -0.076710  0.050174  0.070990 -0.084321   \n",
       "\n",
       "          7         8         9     ...            191       192       193  \\\n",
       "0 -0.003606  0.016723 -0.020666     ...      -0.004500  0.049011  0.009077   \n",
       "1  0.035249 -0.012633  0.067849     ...      -0.008668  0.052009 -0.086604   \n",
       "2  0.000000  0.000000  0.000000     ...       0.000000  0.000000  0.000000   \n",
       "3  0.000482  0.090418 -0.045324     ...       0.101755 -0.028113 -0.107991   \n",
       "4  0.013203  0.058421 -0.024091     ...      -0.006524 -0.048066 -0.103454   \n",
       "\n",
       "        194       195       196       197       198       199  text_author  \n",
       "0 -0.014243  0.005189 -0.018279  0.014966  0.000666 -0.036490      Carroll  \n",
       "1  0.117077 -0.097697 -0.007316  0.021518 -0.009194  0.064450      Carroll  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000      Carroll  \n",
       "3 -0.000094  0.022031 -0.122684 -0.081450 -0.050979 -0.089174      Carroll  \n",
       "4  0.027076 -0.022362 -0.020080  0.108345 -0.062217 -0.022074      Carroll  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_test_lsa = pd.DataFrame(X_test_lsa)\n",
    "alice_test_lsa['text_author'] = 'Carroll'\n",
    "alice_test_lsa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "persuasion = gutenberg.paras('austen-persuasion.txt')\n",
    "#processing\n",
    "persuasion_paras=[]\n",
    "for paragraph in persuasion:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    persuasion_paras.append(' '.join(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1232\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(persuasion_paras, test_size=0.5, random_state=42)\n",
    "\n",
    "#Applying the vectorizer\n",
    "persuasion_paras_tfidf=vectorizer.fit_transform(persuasion_paras)\n",
    "print(\"Number of features: %d\" % persuasion_paras_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(persuasion_paras_tfidf, test_size=0.5, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "X_test_tfidf_csr = X_test_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 75.02858580538904\n"
     ]
    }
   ],
   "source": [
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(200)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "X_test_lsa = lsa.fit_transform(X_test_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>text_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.028985</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>-0.009916</td>\n",
       "      <td>-0.003262</td>\n",
       "      <td>-0.001059</td>\n",
       "      <td>-0.000440</td>\n",
       "      <td>0.077001</td>\n",
       "      <td>-0.030192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077017</td>\n",
       "      <td>-0.018907</td>\n",
       "      <td>-0.042665</td>\n",
       "      <td>0.036893</td>\n",
       "      <td>-0.026422</td>\n",
       "      <td>-0.037499</td>\n",
       "      <td>0.008267</td>\n",
       "      <td>-0.122735</td>\n",
       "      <td>-0.059599</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.004941</td>\n",
       "      <td>0.300561</td>\n",
       "      <td>0.215157</td>\n",
       "      <td>-0.253179</td>\n",
       "      <td>-0.277246</td>\n",
       "      <td>0.218477</td>\n",
       "      <td>0.074005</td>\n",
       "      <td>-0.026749</td>\n",
       "      <td>0.040911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029487</td>\n",
       "      <td>-0.037367</td>\n",
       "      <td>0.031460</td>\n",
       "      <td>0.043598</td>\n",
       "      <td>0.008129</td>\n",
       "      <td>0.027335</td>\n",
       "      <td>0.004534</td>\n",
       "      <td>-0.016467</td>\n",
       "      <td>-0.067264</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.161865</td>\n",
       "      <td>0.034679</td>\n",
       "      <td>-0.026227</td>\n",
       "      <td>0.170084</td>\n",
       "      <td>-0.084102</td>\n",
       "      <td>-0.144933</td>\n",
       "      <td>-0.068405</td>\n",
       "      <td>0.392112</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018358</td>\n",
       "      <td>0.023596</td>\n",
       "      <td>-0.011063</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>-0.047888</td>\n",
       "      <td>-0.009171</td>\n",
       "      <td>-0.050181</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>-0.066271</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.005157</td>\n",
       "      <td>0.308129</td>\n",
       "      <td>0.020142</td>\n",
       "      <td>-0.069271</td>\n",
       "      <td>0.134080</td>\n",
       "      <td>-0.206011</td>\n",
       "      <td>-0.154493</td>\n",
       "      <td>0.037228</td>\n",
       "      <td>-0.273081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019540</td>\n",
       "      <td>-0.057349</td>\n",
       "      <td>0.041422</td>\n",
       "      <td>0.071873</td>\n",
       "      <td>-0.015806</td>\n",
       "      <td>0.047313</td>\n",
       "      <td>0.123111</td>\n",
       "      <td>0.018489</td>\n",
       "      <td>-0.017349</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001251</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>0.117662</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>-0.021270</td>\n",
       "      <td>0.059162</td>\n",
       "      <td>-0.090198</td>\n",
       "      <td>-0.035715</td>\n",
       "      <td>-0.009480</td>\n",
       "      <td>-0.090004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067903</td>\n",
       "      <td>0.053023</td>\n",
       "      <td>0.009821</td>\n",
       "      <td>-0.008757</td>\n",
       "      <td>-0.052598</td>\n",
       "      <td>-0.094452</td>\n",
       "      <td>0.008935</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>-0.045557</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.000176  0.000503  0.028985  0.001404 -0.009916 -0.003262 -0.001059   \n",
       "1  0.001887  0.004941  0.300561  0.215157 -0.253179 -0.277246  0.218477   \n",
       "2  0.001159  0.002691  0.161865  0.034679 -0.026227  0.170084 -0.084102   \n",
       "3  0.003516  0.005157  0.308129  0.020142 -0.069271  0.134080 -0.206011   \n",
       "4  0.001251  0.001968  0.117662  0.001640 -0.021270  0.059162 -0.090198   \n",
       "\n",
       "          7         8         9     ...            191       192       193  \\\n",
       "0 -0.000440  0.077001 -0.030192     ...       0.077017 -0.018907 -0.042665   \n",
       "1  0.074005 -0.026749  0.040911     ...       0.029487 -0.037367  0.031460   \n",
       "2 -0.144933 -0.068405  0.392112     ...      -0.018358  0.023596 -0.011063   \n",
       "3 -0.154493  0.037228 -0.273081     ...       0.019540 -0.057349  0.041422   \n",
       "4 -0.035715 -0.009480 -0.090004     ...       0.067903  0.053023  0.009821   \n",
       "\n",
       "        194       195       196       197       198       199  text_author  \n",
       "0  0.036893 -0.026422 -0.037499  0.008267 -0.122735 -0.059599       Austen  \n",
       "1  0.043598  0.008129  0.027335  0.004534 -0.016467 -0.067264       Austen  \n",
       "2  0.002261 -0.047888 -0.009171 -0.050181  0.000178 -0.066271       Austen  \n",
       "3  0.071873 -0.015806  0.047313  0.123111  0.018489 -0.017349       Austen  \n",
       "4 -0.008757 -0.052598 -0.094452  0.008935  0.003565 -0.045557       Austen  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persuasion_train_lsa = pd.DataFrame(X_train_lsa)\n",
    "persuasion_train_lsa['text_author'] = 'Austen'\n",
    "persuasion_train_lsa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>text_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.056061</td>\n",
       "      <td>0.012687</td>\n",
       "      <td>0.016804</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>-0.026184</td>\n",
       "      <td>0.038055</td>\n",
       "      <td>0.024201</td>\n",
       "      <td>-0.061916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038275</td>\n",
       "      <td>-0.055809</td>\n",
       "      <td>0.031991</td>\n",
       "      <td>-0.044238</td>\n",
       "      <td>0.060893</td>\n",
       "      <td>-0.052125</td>\n",
       "      <td>-0.005878</td>\n",
       "      <td>0.029580</td>\n",
       "      <td>-0.033452</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.239841</td>\n",
       "      <td>-0.037842</td>\n",
       "      <td>-0.046157</td>\n",
       "      <td>0.110368</td>\n",
       "      <td>-0.042889</td>\n",
       "      <td>0.033511</td>\n",
       "      <td>0.154253</td>\n",
       "      <td>0.141901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040582</td>\n",
       "      <td>-0.000318</td>\n",
       "      <td>-0.017065</td>\n",
       "      <td>-0.082850</td>\n",
       "      <td>0.020390</td>\n",
       "      <td>-0.061947</td>\n",
       "      <td>-0.084601</td>\n",
       "      <td>0.003528</td>\n",
       "      <td>-0.041646</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.130648</td>\n",
       "      <td>-0.056647</td>\n",
       "      <td>-0.002846</td>\n",
       "      <td>-0.051495</td>\n",
       "      <td>0.133355</td>\n",
       "      <td>0.016297</td>\n",
       "      <td>0.018107</td>\n",
       "      <td>-0.036465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092918</td>\n",
       "      <td>-0.127945</td>\n",
       "      <td>-0.048630</td>\n",
       "      <td>0.144823</td>\n",
       "      <td>0.008685</td>\n",
       "      <td>0.139263</td>\n",
       "      <td>0.022872</td>\n",
       "      <td>0.011605</td>\n",
       "      <td>0.077892</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003221</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.073068</td>\n",
       "      <td>-0.034655</td>\n",
       "      <td>0.072297</td>\n",
       "      <td>0.036818</td>\n",
       "      <td>-0.054865</td>\n",
       "      <td>-0.001599</td>\n",
       "      <td>0.018144</td>\n",
       "      <td>0.151855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016082</td>\n",
       "      <td>0.036431</td>\n",
       "      <td>0.015157</td>\n",
       "      <td>-0.010048</td>\n",
       "      <td>-0.015901</td>\n",
       "      <td>-0.000557</td>\n",
       "      <td>-0.000324</td>\n",
       "      <td>-0.024960</td>\n",
       "      <td>0.036742</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.085027</td>\n",
       "      <td>0.018744</td>\n",
       "      <td>-0.044631</td>\n",
       "      <td>-0.071234</td>\n",
       "      <td>-0.071327</td>\n",
       "      <td>-0.103052</td>\n",
       "      <td>0.022128</td>\n",
       "      <td>0.046562</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037153</td>\n",
       "      <td>-0.066149</td>\n",
       "      <td>0.063862</td>\n",
       "      <td>-0.043340</td>\n",
       "      <td>-0.027701</td>\n",
       "      <td>0.018389</td>\n",
       "      <td>0.149017</td>\n",
       "      <td>0.042943</td>\n",
       "      <td>-0.035014</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.000489  0.000035  0.056061  0.012687  0.016804  0.000325 -0.026184   \n",
       "1  0.002219  0.000247  0.239841 -0.037842 -0.046157  0.110368 -0.042889   \n",
       "2  0.001349  0.000081  0.130648 -0.056647 -0.002846 -0.051495  0.133355   \n",
       "3  0.003221  0.000042  0.073068 -0.034655  0.072297  0.036818 -0.054865   \n",
       "4  0.000954  0.000052  0.085027  0.018744 -0.044631 -0.071234 -0.071327   \n",
       "\n",
       "          7         8         9     ...            191       192       193  \\\n",
       "0  0.038055  0.024201 -0.061916     ...       0.038275 -0.055809  0.031991   \n",
       "1  0.033511  0.154253  0.141901     ...       0.040582 -0.000318 -0.017065   \n",
       "2  0.016297  0.018107 -0.036465     ...       0.092918 -0.127945 -0.048630   \n",
       "3 -0.001599  0.018144  0.151855     ...       0.016082  0.036431  0.015157   \n",
       "4 -0.103052  0.022128  0.046562     ...      -0.037153 -0.066149  0.063862   \n",
       "\n",
       "        194       195       196       197       198       199  text_author  \n",
       "0 -0.044238  0.060893 -0.052125 -0.005878  0.029580 -0.033452       Austen  \n",
       "1 -0.082850  0.020390 -0.061947 -0.084601  0.003528 -0.041646       Austen  \n",
       "2  0.144823  0.008685  0.139263  0.022872  0.011605  0.077892       Austen  \n",
       "3 -0.010048 -0.015901 -0.000557 -0.000324 -0.024960  0.036742       Austen  \n",
       "4 -0.043340 -0.027701  0.018389  0.149017  0.042943 -0.035014       Austen  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persuasion_test_lsa = pd.DataFrame(X_test_lsa)\n",
    "persuasion_test_lsa['text_author'] = 'Austen'\n",
    "persuasion_test_lsa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lsa = pd.concat([alice_train_lsa,persuasion_train_lsa])\n",
    "test_lsa = pd.concat([alice_test_lsa,persuasion_test_lsa])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Author Using Various Models and Feature Sets\n",
    "\n",
    "I will now test my ability to predict author from text. There are many different types of models with many different usesm but I will try 4 different ones here: Logistic Regression, Random Forest, Grandient-Boosted Decision Trees, and Support Vector Classifier. \n",
    "\n",
    "Additionally, I will be modeling with both of my features sets.\n",
    "\n",
    "I will check each models cross validation score to check the overall health of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words Feature Set\n",
    "X_bow = total_word_counts.drop(['text_author','text_sentence'],1)\n",
    "y_bow = total_word_counts.text_author\n",
    "Xtrain_bow, Xtest_bow, ytrain_bow, ytest_bow = train_test_split(X_bow,y_bow,test_size=0.5,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244, 1635)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA Reduced Feature Set\n",
    "Xtrain_lsa = train_lsa.drop('text_author',1)\n",
    "ytrain_lsa = train_lsa.text_author\n",
    "Xtest_lsa = test_lsa.drop('text_author',1)\n",
    "ytest_lsa = test_lsa.text_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(924, 200)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_lsa.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9877049180327869\n",
      "\n",
      "Test set score: 0.9020408163265307\n"
     ]
    }
   ],
   "source": [
    "lr_bow = LogisticRegression(random_state=42)\n",
    "lr_bow.fit(Xtrain_bow, ytrain_bow)\n",
    "print('Training set score:', lr_bow.score(Xtrain_bow, ytrain_bow))\n",
    "print('\\nTest set score:', lr_bow.score(Xtest_bow, ytest_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_bow_cv = cross_val_score(lr_bow, Xtest_bow, ytest_bow, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Cross Validation\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.96      , 0.88      , 0.92      , 0.8       , 0.8       ,\n",
       "       0.84      , 0.76      , 0.83333333, 0.95652174, 0.86956522])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean and Standard Error:\n",
      "0.862  +/-  0.129\n"
     ]
    }
   ],
   "source": [
    "print('Logistic Regression Cross Validation\\n')\n",
    "display(lr_bow_cv)\n",
    "print('\\nMean and Standard Error:')\n",
    "print(round(lr_bow_cv.mean(),3),' +/- ',round(lr_bow_cv.std()*2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8398268398268398\n",
      "\n",
      "Test set score: 0.787027027027027\n"
     ]
    }
   ],
   "source": [
    "lr_lsa = LogisticRegression(random_state=42)\n",
    "lr_lsa.fit(Xtrain_lsa, ytrain_lsa)\n",
    "print('Training set score:', lr_lsa.score(Xtrain_lsa, ytrain_lsa))\n",
    "print('\\nTest set score:', lr_lsa.score(Xtest_lsa, ytest_lsa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_lsa_cv = cross_val_score(lr_lsa, Xtest_lsa, ytest_lsa, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Cross Validation\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.91397849, 0.87096774, 0.83870968, 0.84946237, 0.74193548,\n",
       "       0.86021505, 0.86956522, 0.82608696, 0.88043478, 0.81318681])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean and Standard Error:\n",
      "0.846  +/-  0.088\n"
     ]
    }
   ],
   "source": [
    "print('Logistic Regression Cross Validation\\n')\n",
    "display(lr_lsa_cv)\n",
    "print('\\nMean and Standard Error:')\n",
    "print(round(lr_lsa_cv.mean(),3),' +/- ',round(lr_lsa_cv.std()*2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-Boosted Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9959016393442623\n",
      "\n",
      "Test set score: 0.8653061224489796\n"
     ]
    }
   ],
   "source": [
    "clf_bow = GradientBoostingClassifier(random_state=42)\n",
    "clf_bow.fit(Xtrain_bow, ytrain_bow)\n",
    "print('Training set score:', clf_bow.score(Xtrain_bow, ytrain_bow))\n",
    "print('\\nTest set score:', clf_bow.score(Xtest_bow, ytest_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_bow_cv = cross_val_score(clf_bow, Xtest_bow, ytest_bow, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient-Boosted Tree Cross Validation\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.88      , 0.92      , 0.8       , 0.8       , 0.84      ,\n",
       "       0.92      , 0.8       , 0.79166667, 0.7826087 , 0.86956522])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean and Standard Error:\n",
      "0.84  +/-  0.101\n"
     ]
    }
   ],
   "source": [
    "print('Gradient-Boosted Tree Cross Validation\\n')\n",
    "display(clf_bow_cv)\n",
    "print('\\nMean and Standard Error:')\n",
    "print(round(clf_bow_cv.mean(),3),' +/- ',round(clf_bow_cv.std()*2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9902597402597403\n",
      "\n",
      "Test set score: 0.9567567567567568\n"
     ]
    }
   ],
   "source": [
    "clf_lsa = GradientBoostingClassifier(random_state=42)\n",
    "clf_lsa.fit(Xtrain_lsa, ytrain_lsa)\n",
    "print('Training set score:', clf_lsa.score(Xtrain_lsa, ytrain_lsa))\n",
    "print('\\nTest set score:', clf_lsa.score(Xtest_lsa, ytest_lsa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lsa_cv = cross_val_score(clf_lsa, Xtest_lsa, ytest_lsa, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient-Boosted Tree Cross Validation\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.98924731, 1.        , 1.        , 1.        , 0.98924731,\n",
       "       0.96774194, 0.9673913 , 0.94565217, 0.98913043, 0.96703297])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean and Standard Error:\n",
      "0.982  +/-  0.035\n"
     ]
    }
   ],
   "source": [
    "print('Gradient-Boosted Tree Cross Validation\\n')\n",
    "display(clf_lsa_cv)\n",
    "print('\\nMean and Standard Error:')\n",
    "print(round(clf_lsa_cv.mean(),3),' +/- ',round(clf_lsa_cv.std()*2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.7254098360655737\n",
      "\n",
      "Test set score: 0.689795918367347\n"
     ]
    }
   ],
   "source": [
    "rfc_bow = RandomForestClassifier(max_depth=3,random_state=42)\n",
    "rfc_bow.fit(Xtrain_bow, ytrain_bow)\n",
    "print('Training set score:', rfc_bow.score(Xtrain_bow, ytrain_bow))\n",
    "print('\\nTest set score:', rfc_bow.score(Xtest_bow, ytest_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_bow_cv = cross_val_score(rfc_bow, Xtest_bow, ytest_bow, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Cross Validation\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.68      , 0.68      , 0.64      , 0.72      , 0.68      ,\n",
       "       0.64      , 0.72      , 0.66666667, 0.65217391, 0.69565217])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean and Standard Error:\n",
      "0.677  +/-  0.055\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest Cross Validation\\n')\n",
    "display(rfc_bow_cv)\n",
    "print('\\nMean and Standard Error:')\n",
    "print(round(rfc_bow_cv.mean(),3),' +/- ',round(rfc_bow_cv.std()*2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9285714285714286\n",
      "\n",
      "Test set score: 0.8789189189189189\n"
     ]
    }
   ],
   "source": [
    "rfc_lsa = RandomForestClassifier(max_depth=3,random_state=42)\n",
    "rfc_lsa.fit(Xtrain_lsa, ytrain_lsa)\n",
    "print('Training set score:', rfc_lsa.score(Xtrain_lsa, ytrain_lsa))\n",
    "print('\\nTest set score:', rfc_lsa.score(Xtest_lsa, ytest_lsa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_lsa_cv = cross_val_score(rfc_lsa, Xtest_lsa, ytest_lsa, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Cross Validation\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.92473118, 0.96774194, 0.94623656, 0.97849462, 0.97849462,\n",
       "       0.95698925, 0.85869565, 0.94565217, 0.94565217, 0.94505495])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean and Standard Error:\n",
      "0.945  +/-  0.066\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest Cross Validation\\n')\n",
    "display(rfc_lsa_cv)\n",
    "print('\\nMean and Standard Error:')\n",
    "print(round(rfc_lsa_cv.mean(),3),' +/- ',round(rfc_lsa_cv.std()*2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.6639344262295082\n",
      "\n",
      "Test set score: 0.6448979591836734\n"
     ]
    }
   ],
   "source": [
    "svc_bow = SVC(random_state=42)\n",
    "svc_bow.fit(Xtrain_bow, ytrain_bow)\n",
    "print('Training set score:', svc_bow.score(Xtrain_bow, ytrain_bow))\n",
    "print('\\nTest set score:', svc_bow.score(Xtest_bow, ytest_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_bow_cv = cross_val_score(svc_bow, Xtest_bow, ytest_bow, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Cross Validation\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.64      , 0.64      , 0.64      , 0.64      , 0.64      ,\n",
       "       0.64      , 0.64      , 0.66666667, 0.65217391, 0.65217391])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean and Standard Error:\n",
      "0.645  +/-  0.017\n"
     ]
    }
   ],
   "source": [
    "print('Support Vector Machine Cross Validation\\n')\n",
    "display(svc_bow_cv)\n",
    "print('\\nMean and Standard Error:')\n",
    "print(round(svc_bow_cv.mean(),3),' +/- ',round(svc_bow_cv.std()*2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.5584415584415584\n",
      "\n",
      "Test set score: 0.5578378378378378\n"
     ]
    }
   ],
   "source": [
    "svc_lsa = SVC(random_state=42)\n",
    "svc_lsa.fit(Xtrain_lsa, ytrain_lsa)\n",
    "print('Training set score:', svc_lsa.score(Xtrain_lsa, ytrain_lsa))\n",
    "print('\\nTest set score:', svc_lsa.score(Xtest_lsa, ytest_lsa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_lsa_cv = cross_val_score(svc_lsa, Xtest_bow, ytest_bow, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Cross Validation\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.64      , 0.64      , 0.64      , 0.64      , 0.64      ,\n",
       "       0.64      , 0.64      , 0.66666667, 0.65217391, 0.65217391])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean and Standard Error:\n",
      "0.645  +/-  0.017\n"
     ]
    }
   ],
   "source": [
    "print('Support Vector Machine Cross Validation\\n')\n",
    "display(svc_lsa_cv)\n",
    "print('\\nMean and Standard Error:')\n",
    "print(round(svc_lsa_cv.mean(),3),' +/- ',round(svc_lsa_cv.std()*2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Conclusion\n",
    "\n",
    "The best model to model the BOW feature set was Logistic Regression, having the highest accuracy score and a fairly stable cross valiadation score.\n",
    "\n",
    "The best model to predict with the TF IDF feature set was the Random Forest Classifier. \n",
    "\n",
    "Though Logistic Regression using BOW Features had the best testing score, the Random Forest using the TF IDF features showed a more stable cross validation score. Because of that, I will treat both of them as the most effective and try to improve on each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the Best Models Better\n",
    "\n",
    "I want to squeeze even more accuracy out of my models by tweaking their hyper parameters to better fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression using BOW Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C value is  0.01\n",
      "Best C value is  0.1\n",
      "Best C value is  1\n",
      "Best C value is  5\n",
      "Best C value is  8\n"
     ]
    }
   ],
   "source": [
    "constants = [.01,.1,1,5,7,8,9,10,11,12,15,20,30,50,70,100,150,200]\n",
    "bestc = 0\n",
    "bestscore = 0\n",
    "for c in constants:\n",
    "    lr2 = LogisticRegression(C=c,random_state=42)\n",
    "    lr2.fit(Xtrain_bow, ytrain_bow)\n",
    "    score = lr2.score(Xtest_bow, ytest_bow)\n",
    "    if score > bestscore:\n",
    "        bestc = c\n",
    "        bestscore = score\n",
    "        print('Best C value is ',c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2_bow = LogisticRegression(C=bestc,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9959016393442623\n",
      "\n",
      "Test set score: 0.926530612244898\n"
     ]
    }
   ],
   "source": [
    "lr2_bow.fit(Xtrain_bow, ytrain_bow)\n",
    "print('Training set score:', lr2_bow.score(Xtrain_bow, ytrain_bow))\n",
    "print('\\nTest set score:', lr2_bow.score(Xtest_bow, ytest_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2_bow_cv = cross_val_score(lr2_bow, Xtest_bow, ytest_bow, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Cross Validation\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.96      , 0.88      , 0.92      , 0.84      , 0.8       ,\n",
       "       0.84      , 0.76      , 0.79166667, 0.95652174, 0.86956522])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean and Standard Error:\n",
      "0.862  +/-  0.13\n"
     ]
    }
   ],
   "source": [
    "print('Logistic Regression Cross Validation\\n')\n",
    "display(lr2_bow_cv)\n",
    "print('\\nMean and Standard Error:')\n",
    "print(round(lr2_bow_cv.mean(),3),' +/- ',round(lr2_bow_cv.std()*2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest using TF IDF Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9664502164502164\n",
      "\n",
      "Test set score: 0.918918918918919\n"
     ]
    }
   ],
   "source": [
    "rfc2_lsa = RandomForestClassifier(n_estimators=100,max_depth=3,random_state=42)\n",
    "rfc2_lsa.fit(Xtrain_lsa, ytrain_lsa)\n",
    "print('Training set score:', rfc2_lsa.score(Xtrain_lsa, ytrain_lsa))\n",
    "print('\\nTest set score:', rfc2_lsa.score(Xtest_lsa, ytest_lsa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc2_lsa_cv = cross_val_score(rfc2_lsa, Xtest_lsa, ytest_lsa, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Cross Validation\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 1.        , 1.        , 0.98924731,\n",
       "       0.97849462, 0.90217391, 0.95652174, 0.98913043, 0.96703297])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean and Standard Error:\n",
      "0.978  +/-  0.058\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest Cross Validation\\n')\n",
    "display(rfc2_lsa_cv)\n",
    "print('\\nMean and Standard Error:')\n",
    "print(round(rfc2_lsa_cv.mean(),3),' +/- ',round(rfc2_lsa_cv.std()*2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Improvement Conclusion\n",
    "\n",
    "I was able to imporve the Logistic Regression test score by 2.5%, as well as making the cross validation far more stable.\n",
    "\n",
    "I was able to imporve the Random Forest Classifier test score by 2.4%, as well as making the cross valiation mean stightly more stable.\n",
    "\n",
    "In the end, Logistic Regression benefitted from the optimization slightly more, and is the slightly better model overall. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
