{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Author From Text\n",
    "\n",
    "I want to make a model that can predict who has written a novel or other text based on features that I can parse form the text.  To accomplish this, I will use data from the Gutenberg corpus from NLTK, or the Natural Language Toolkit.  This is a collection of many free text corpra for those looking to analyze texts without having to pay for a corpra to analyze.\n",
    "\n",
    "I will begin by importing my data.\n",
    "\n",
    "I will use ten different texts from ten different authors: _Persuasion_ by Jane Austion, the poems of William Blake, the stories of William Cullen Bryant, _Buster Bear_ by Thornton Burgess, _Alice in Wonderland_ by Lewis Carroll, _The Man Who Became Thursday_ by G. K. Chesterton, _The Parent's Assistant_ by Maria Edgeworth, _Moby Dick_ by Herman Melville, _Hamelt_ by William Shakespeare, and _Leaves of Grass_ by Walt Whitman. \n",
    "\n",
    "This gives me quite a diverse set of texts and authors, having 4 novels, 2 collections of poems, 3 collections of short stories, and 1 play. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Cleaning\n",
    "\n",
    "I will create features using Term Frequency and Inverse Document Frequency analysis. This will be best for my data because different authors structure their writing differently and express different themes, something TDIDF can learn and model.\n",
    "\n",
    "I will import I text for each author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "persuasion = gutenberg.paras('austen-persuasion.txt')\n",
    "#processing\n",
    "persuasion_paras=[]\n",
    "for paragraph in persuasion:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    persuasion_paras.append(' '.join(para))\n",
    "    \n",
    "persuasion_len = len(persuasion_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "blake = gutenberg.paras('blake-poems.txt')\n",
    "#processing\n",
    "blake_paras=[]\n",
    "for paragraph in blake:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    blake_paras.append(' '.join(para))\n",
    "    \n",
    "blake_len = len(blake_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bryant = gutenberg.paras('bryant-stories.txt')\n",
    "#processing\n",
    "bryant_paras=[]\n",
    "for paragraph in bryant:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    bryant_paras.append(' '.join(para))\n",
    "    \n",
    "bryant_len = len(bryant_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "buster = gutenberg.paras('burgess-busterbrown.txt')\n",
    "#processing\n",
    "buster_paras=[]\n",
    "for paragraph in buster:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    buster_paras.append(' '.join(para))\n",
    "    \n",
    "buster_len = len(buster_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = gutenberg.paras('carroll-alice.txt')\n",
    "#processing\n",
    "alice_paras=[]\n",
    "for paragraph in alice:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    alice_paras.append(' '.join(para))\n",
    "    \n",
    "alice_len = len(alice_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "thursday = gutenberg.paras('chesterton-thursday.txt')\n",
    "#processing\n",
    "thursday_paras=[]\n",
    "for paragraph in thursday:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    thursday_paras.append(' '.join(para))\n",
    "    \n",
    "thursday_len = len(thursday_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "parents = gutenberg.paras('edgeworth-parents.txt')\n",
    "#processing\n",
    "parents_paras=[]\n",
    "for paragraph in parents:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    parents_paras.append(' '.join(para))\n",
    "    \n",
    "parents_len = len(parents_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby = gutenberg.paras('melville-moby_dick.txt')\n",
    "#processing\n",
    "moby_paras=[]\n",
    "for paragraph in moby:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    moby_paras.append(' '.join(para))\n",
    "    \n",
    "moby_len = len(moby_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet = gutenberg.paras('shakespeare-hamlet.txt')\n",
    "\n",
    "#processing\n",
    "shake_paras=[]\n",
    "for paragraph in hamlet:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    para=[re.sub(r'Actus [A-Z][a-z]*\\.','',word) for word in para] # Remove act numbers\n",
    "    para=[re.sub(r'Scoena [A-Z][a-z]*\\.','',word) for word in para] # Remove scene numbers\n",
    "    para=[re.sub(r'[A-Z][a-z]*\\s\\.','',word) for word in para] # Remove charactr's names before their lines\n",
    "    para=[re.sub(r'[A-Z][a-z]*\\s\\.','',word) for word in para] # Remove charactr's names before their lines\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    shake_paras.append(' '.join(para))\n",
    "    \n",
    "\n",
    "    \n",
    "shake_len = len(shake_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves = gutenberg.paras('whitman-leaves.txt')\n",
    "#processing\n",
    "leaves_paras=[]\n",
    "for paragraph in leaves:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    leaves_paras.append(' '.join(para))\n",
    "    \n",
    "leaves_len = len(leaves_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paras = persuasion_paras + blake_paras + bryant_paras + buster_paras + alice_paras + thursday_paras + parents_paras + moby_paras + shake_paras + leaves_paras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating TFIDF Features\n",
    "\n",
    "Now that all my data has been imported and stored, I can transform it into TFIDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create author labels for text\n",
    "labels = []\n",
    "for i in range(persuasion_len):\n",
    "    labels.append('Austen')\n",
    "for i in range(blake_len):\n",
    "    labels.append('Blake')\n",
    "for i in range(bryant_len):\n",
    "    labels.append('Bryant')\n",
    "for i in range(buster_len):\n",
    "    labels.append('Burgess')\n",
    "for i in range(alice_len):\n",
    "    labels.append('Carroll')\n",
    "for i in range(thursday_len):\n",
    "    labels.append('Chesterton')\n",
    "for i in range(parents_len):\n",
    "    labels.append('Edgeworth')\n",
    "for i in range(moby_len):\n",
    "    labels.append('Melville')\n",
    "for i in range(shake_len):\n",
    "    labels.append('Shakespeare')\n",
    "for i in range(leaves_len):\n",
    "    labels.append('Whitman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 6986\n"
     ]
    }
   ],
   "source": [
    "#splitting into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(all_paras, labels, test_size=0.5, random_state=0)\n",
    "\n",
    "#Applying the vectorizer\n",
    "X_train_tfidf=vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf=vectorizer.transform(X_test)\n",
    "print(\"Number of features: %d\" % X_train_tfidf.get_shape()[1])\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "X_test_tfidf_csr = X_test_tfidf.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: Supper over , the company went back to the bar - room , when , knowing not what else to do with myself , I resolved to spend the rest of the evening as a looker on .\n",
      "Tf_idf vector: {'supper': 0.3190147090269327, 'company': 0.306504947382207, 'went': 0.23746727833213563, 'bar': 0.3458769414258709, 'room': 0.2666127080621043, 'knowing': 0.36319005599692344, 'resolved': 0.3537421348086776, 'spend': 0.38228005006490146, 'rest': 0.26938301991861796, 'evening': 0.28536759951639357}\n"
     ]
    }
   ],
   "source": [
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "    \n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 46.00092644260595\n"
     ]
    }
   ],
   "source": [
    "#Our SVD data reducer.  We are going to reduce the feature space from 6986 to 400\n",
    "svd= TruncatedSVD(400, random_state=0)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "X_test_lsa = lsa.transform(X_test_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Component 0:\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Component 1:\n",
      "CHAPTER 64     0.999719\n",
      "Chapter 2      0.999719\n",
      "Chapter 5      0.999719\n",
      "Chapter 9      0.999719\n",
      "CHAPTER 96     0.999719\n",
      "CHAPTER 132    0.999719\n",
      "CHAPTER 114    0.999719\n",
      "CHAPTER 77     0.999719\n",
      "CHAPTER 103    0.999719\n",
      "CHAPTER 94     0.999719\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "Component 2:\n",
      "\" Oh , at Harrow ,\" said the policeman                                                         0.735348\n",
      "\" Now you do that ,\" said he .                                                                 0.671664\n",
      "\" I have done it ,\" he said hoarsely .                                                         0.662345\n",
      "Whoever degrades another degrades me , And whatever is done or said returns at last to me .    0.655795\n",
      "\" They have done us ,\" he said , with brief military irony .                                   0.648707\n",
      "\" It is jolly to get some pals ,\" he said .                                                    0.643445\n",
      "\" I do ,\" said the other \" martyrs .                                                           0.642135\n",
      "\" Not I ,\" said the Goose .                                                                    0.623437\n",
      "\" You will not ,\" said the giant .                                                             0.613023\n",
      "\" You said I was not serious about being an anarchist .\"                                       0.610519\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "Component 3:\n",
      "\" Oh , no !                              0.873917\n",
      "\" Oh !                                   0.873917\n",
      "\" Oh !                                   0.873917\n",
      "\" Oh , EDICATION !                       0.873917\n",
      "\" Oh !                                   0.873917\n",
      "\" Oh !\"                                  0.873917\n",
      "\" Oh !                                   0.873917\n",
      "Oh !                                     0.873917\n",
      "\" Oh !                                   0.873917\n",
      "\" And he have one , two , three  oh !    0.873917\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "Component 4:\n",
      "King .    0.969387\n",
      "King .    0.969387\n",
      "King .    0.969387\n",
      "King .    0.969387\n",
      "King .    0.969387\n",
      "King .    0.969387\n",
      "King .    0.969387\n",
      "King .    0.969387\n",
      "King .    0.969387\n",
      "King .    0.969387\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('\\nComponent {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I have shown the first 5 components by the entries that best exemplify them.  The left hand value is the sentence and the right hand value is how strongly the sentence correlates with that particular component.\n",
    "\n",
    "It seems that compenent 0 targets the Hamlet's line prompts.\n",
    "\n",
    "Component 1 targets Chapter titles which start with the word 'chapter'.\n",
    "\n",
    "Component 2 seems to target sentences where one persons talks about the effect of another character's actions.\n",
    "\n",
    "Component 3 targets exclamatory sentences begining with 'Oh'.\n",
    "\n",
    "And component 4 targets the begining of the King's line prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Clusters\n",
    "\n",
    "I will now cluster my data and see if it will naturally group by author.\n",
    "\n",
    "I will use the KMeans clustering technique, as it is the most flexible and least computationally intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "km1 = KMeans(n_clusters=10, random_state=0)\n",
    "y_pred1 = km1.fit_predict(X_train_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "km2 = KMeans(n_clusters=10, random_state=42)\n",
    "y_pred2 = km2.fit_predict(X_train_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "km3 = KMeans(n_clusters=10, random_state=1337)\n",
    "y_pred3 = km3.fit_predict(X_train_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans 1 Inertia:  6588.094229689489\n",
      "\n",
      "Comparing the K-Means 1 clusters to the actual author groupings:\n",
      "\n",
      "col_0         0    1    2   3    4     5   6   7    8   9\n",
      "row_0                                                    \n",
      "Austen       50   25   19  10   28   374  15  15    0   0\n",
      "Blake         0   31   14   4    2    77   0   4    0   0\n",
      "Bryant        2   65  132   3   70   274   0  24    0  24\n",
      "Burgess       2   10   28   0    4    83   0   3    0   0\n",
      "Carroll       0   31   27   4  116   226   4   4    0  27\n",
      "Chesterton    5   44   17  11  163   358   9  49    0   0\n",
      "Edgeworth    91  101   72  33  283  1186   4  68    0   0\n",
      "Melville      7  427   26  13   66   706  72  78    0   3\n",
      "Shakespeare  26   30    0   1    0   216   0  25  141  43\n",
      "Whitman       0  474   19  47    3   575   0  91    0   0\n"
     ]
    }
   ],
   "source": [
    "print('KMeans 1 Inertia: ',km1.inertia_)\n",
    "print('\\nComparing the K-Means 1 clusters to the actual author groupings:\\n')\n",
    "print(pd.crosstab(Y_train,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans 2 Inertia:  6572.738657087793\n",
      "\n",
      "Comparing the K-Means 2 clusters to the actual author groupings:\n",
      "\n",
      "col_0          0   1    2    3   4   5   6    7     8   9\n",
      "row_0                                                    \n",
      "Austen        20  15   30   22  30   0  15    0   389  15\n",
      "Blake         14   1    2   48   2   0   5    0    60   0\n",
      "Bryant       128  14   69   57  13  24  13    0   276   0\n",
      "Burgess       28   1    4    6   5   0   0    0    86   0\n",
      "Carroll       25   1  103   20  36  27   9    0   214   4\n",
      "Chesterton    15  41  163   41  22   0  11    0   354   9\n",
      "Edgeworth     72  45  291   68  97   0  82    0  1179   4\n",
      "Melville      23  61   66  450  31   3  18    0   674  72\n",
      "Shakespeare    0  46    0   46   2  43   1  141   203   0\n",
      "Whitman       19  47    3  637  36   0   0    0   467   0\n"
     ]
    }
   ],
   "source": [
    "print('KMeans 2 Inertia: ',km2.inertia_)\n",
    "print('\\nComparing the K-Means 2 clusters to the actual author groupings:\\n')\n",
    "print(pd.crosstab(Y_train,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans 3 Inertia:  6586.917000775612\n",
      "\n",
      "Comparing the K-Means 3 clusters to the actual author groupings:\n",
      "\n",
      "col_0         0    1    2    3   4   5    6    7     8   9\n",
      "row_0                                                     \n",
      "Austen        0   21    0    0  14  19   49   24   394  15\n",
      "Blake         0   14    0    0   1   0    2   66    49   0\n",
      "Bryant       22  127    2    0  14  13   76   71   269   0\n",
      "Burgess       0   28    0    0   1   3    4   14    80   0\n",
      "Carroll      27   28    0    0   1   2  121   26   230   4\n",
      "Chesterton    0   15    0    0  42  11  173   59   347   9\n",
      "Edgeworth     0   66    0    0  43  73  325   94  1233   4\n",
      "Melville      2   21  172    0  57  27   73  285   690  71\n",
      "Shakespeare  43    0   26  141   0  30    1    5   236   0\n",
      "Whitman       0   17    0    0  41  20    3  741   387   0\n"
     ]
    }
   ],
   "source": [
    "print('KMeans 3 Inertia: ',km3.inertia_)\n",
    "print('\\nComparing the K-Means 3 clusters to the actual author groupings:\\n')\n",
    "print(pd.crosstab(Y_train,y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My clusters are certainly not grouping by author.  This indicates that the difference between author may not be instantly apparent from the feature set I have created.\n",
    "\n",
    "However, my groupings to seem to be somewhat consistent.  And if the groups are consitently being formed, they must be meaningful in some way.  There is always 8 certain groupings of clusters (given with their cluster number in each KMeans run):\n",
    "\n",
    "__(8, 7, 3)__: a cluster containing exactly 141 Shakespeare entries and nothing else. This is the only cluster that is exactly the same between KMeans runs.\n",
    "\n",
    "__(9, 5, 0)__: a sparsely populated cluster containing only Bryant, Carroll, and Shakespeare.  This cluster is almost exactly the same between KMeans runs, only have a difference of 2 or 3 texts total.\n",
    "\n",
    "__(6, 9, 9)__: A low populated cluster with plurality Melville and Austin as the other author of significant representation. This cluster was extremely similar in count between KMeans runs, though not exactly the same.\n",
    "\n",
    "__(5, 8, 8)__: A heavily populated cluster dominating by Edgeworth but containing a every other author in similar ratios.  This was consistently the highest populated cluster by far.\n",
    "\n",
    "__(2, 0, 1)__: a medium population cluster dominated by Byant but containing all authors.\n",
    "\n",
    "__(4, 2, 6)__: a medium population cluster dominated by Edgeworth followed by Chesterton and Carroll, though it usually contains most authors. Also marked by it's consistently low amount of Blake, Burgess, and Skakespeare. \n",
    "\n",
    "__(7, 1, 4)__: a sparsely populated cluster with pluraity Melville, followed by Edgeworth and Chesterton, though it usually contains all authors.  This cluster is also marked by its consistently low amount of Blake, Burgess, and Carroll.\n",
    "\n",
    "__(1, 3, 7)__: a medium to largely populated cluster dominated by Whitman and Melville though containing all authors. This is the most variable of the recognizable clusters.\n",
    "\n",
    "Though there are some vague similarities between the remaining 2 clusters in each iteration, there are no trends that stay consistent across all runs. These clusters rely on the groupings of the other clusters to form than from an inherently strong grouping amoung the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a Closer Look at Clusters\n",
    "\n",
    "I want to determine how the groups are being formed. To do this, I will look at the most important components behind each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras_by_component_with_label=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "paras_by_component_with_label['Author'] = Y_train\n",
    "paras_by_component_with_label['Predict1'] = y_pred1\n",
    "paras_by_component_with_label['Predict2'] = y_pred2\n",
    "paras_by_component_with_label['Predict3'] = y_pred3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_components(df, ind):\n",
    "    top_components = []\n",
    "    top_tfidf = []\n",
    "    for i in range(ind):\n",
    "        max_tfidf = max(df.loc[:,i])\n",
    "        if (max_tfidf) > 0:\n",
    "            if len(top_components) == 3:\n",
    "                if max_tfidf > min(top_tfidf):\n",
    "                    del top_components[np.argmin(top_tfidf)]\n",
    "                    del top_tfidf[np.argmin(top_tfidf)]\n",
    "                    top_components.append(i)\n",
    "                    top_tfidf.append(max_tfidf)\n",
    "            else:\n",
    "                top_components.append(i)\n",
    "                top_tfidf.append(max_tfidf) \n",
    "    return top_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_factors(index, df):\n",
    "    for i in index:\n",
    "        print('\\nComponent {}:'.format(i))\n",
    "        print(df.loc[:,i].sort_values(ascending=False)[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_10 = paras_by_component_with_label[paras_by_component_with_label['Predict1']==0]\n",
    "cluster_11 = paras_by_component_with_label[paras_by_component_with_label['Predict1']==1]\n",
    "cluster_12 = paras_by_component_with_label[paras_by_component_with_label['Predict1']==2]\n",
    "cluster_13 = paras_by_component_with_label[paras_by_component_with_label['Predict1']==3]\n",
    "cluster_14 = paras_by_component_with_label[paras_by_component_with_label['Predict1']==4]\n",
    "cluster_15 = paras_by_component_with_label[paras_by_component_with_label['Predict1']==5]\n",
    "cluster_16 = paras_by_component_with_label[paras_by_component_with_label['Predict1']==6]\n",
    "cluster_17 = paras_by_component_with_label[paras_by_component_with_label['Predict1']==7]\n",
    "cluster_18 = paras_by_component_with_label[paras_by_component_with_label['Predict1']==8]\n",
    "cluster_19 = paras_by_component_with_label[paras_by_component_with_label['Predict1']==9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_20 = paras_by_component_with_label[paras_by_component_with_label['Predict2']==0]\n",
    "cluster_21 = paras_by_component_with_label[paras_by_component_with_label['Predict2']==1]\n",
    "cluster_22 = paras_by_component_with_label[paras_by_component_with_label['Predict2']==2]\n",
    "cluster_23 = paras_by_component_with_label[paras_by_component_with_label['Predict2']==3]\n",
    "cluster_24 = paras_by_component_with_label[paras_by_component_with_label['Predict2']==4]\n",
    "cluster_25 = paras_by_component_with_label[paras_by_component_with_label['Predict2']==5]\n",
    "cluster_26 = paras_by_component_with_label[paras_by_component_with_label['Predict2']==6]\n",
    "cluster_27 = paras_by_component_with_label[paras_by_component_with_label['Predict2']==7]\n",
    "cluster_28 = paras_by_component_with_label[paras_by_component_with_label['Predict2']==8]\n",
    "cluster_29 = paras_by_component_with_label[paras_by_component_with_label['Predict2']==9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_30 = paras_by_component_with_label[paras_by_component_with_label['Predict3']==0]\n",
    "cluster_31 = paras_by_component_with_label[paras_by_component_with_label['Predict3']==1]\n",
    "cluster_32 = paras_by_component_with_label[paras_by_component_with_label['Predict3']==2]\n",
    "cluster_33 = paras_by_component_with_label[paras_by_component_with_label['Predict3']==3]\n",
    "cluster_34 = paras_by_component_with_label[paras_by_component_with_label['Predict3']==4]\n",
    "cluster_35 = paras_by_component_with_label[paras_by_component_with_label['Predict3']==5]\n",
    "cluster_36 = paras_by_component_with_label[paras_by_component_with_label['Predict3']==6]\n",
    "cluster_37 = paras_by_component_with_label[paras_by_component_with_label['Predict3']==7]\n",
    "cluster_38 = paras_by_component_with_label[paras_by_component_with_label['Predict3']==8]\n",
    "cluster_39 = paras_by_component_with_label[paras_by_component_with_label['Predict3']==9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_10 = find_top_components(cluster_10, 300)\n",
    "comp_11 = find_top_components(cluster_11, 300)\n",
    "comp_12 = find_top_components(cluster_12, 300)\n",
    "comp_13 = find_top_components(cluster_13, 300)\n",
    "comp_14 = find_top_components(cluster_14, 300)\n",
    "comp_15 = find_top_components(cluster_15, 300)\n",
    "comp_16 = find_top_components(cluster_16, 300)\n",
    "comp_17 = find_top_components(cluster_17, 300)\n",
    "comp_18 = find_top_components(cluster_18, 300)\n",
    "comp_19 = find_top_components(cluster_19, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_20 = find_top_components(cluster_20, 300)\n",
    "comp_21 = find_top_components(cluster_21, 300)\n",
    "comp_22 = find_top_components(cluster_22, 300)\n",
    "comp_23 = find_top_components(cluster_23, 300)\n",
    "comp_24 = find_top_components(cluster_24, 300)\n",
    "comp_25 = find_top_components(cluster_25, 300)\n",
    "comp_26 = find_top_components(cluster_26, 300)\n",
    "comp_27 = find_top_components(cluster_27, 300)\n",
    "comp_28 = find_top_components(cluster_28, 300)\n",
    "comp_29 = find_top_components(cluster_29, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_30 = find_top_components(cluster_30, 300)\n",
    "comp_31 = find_top_components(cluster_31, 300)\n",
    "comp_32 = find_top_components(cluster_32, 300)\n",
    "comp_33 = find_top_components(cluster_33, 300)\n",
    "comp_34 = find_top_components(cluster_34, 300)\n",
    "comp_35 = find_top_components(cluster_35, 300)\n",
    "comp_36 = find_top_components(cluster_36, 300)\n",
    "comp_37 = find_top_components(cluster_37, 300)\n",
    "comp_38 = find_top_components(cluster_38, 300)\n",
    "comp_39 = find_top_components(cluster_39, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8, 7, 3): 141 Skakespeare\n",
    "\n",
    "This was the only cluster to stay exactly the same amoung clustering, so it's grouping must be strong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 1, Cluster 8:\n",
      "\n",
      "Component 0:\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Component 205:\n",
      "Ham .    1.406077e-13\n",
      "Ham .    1.397978e-13\n",
      "Ham .    1.391472e-13\n",
      "Ham .    1.391472e-13\n",
      "Ham .    1.391472e-13\n",
      "Name: 205, dtype: float64\n",
      "\n",
      "Component 293:\n",
      "Ham .    1.326131e-13\n",
      "Ham .    1.295566e-13\n",
      "Ham .    1.262961e-13\n",
      "Ham .    1.262875e-13\n",
      "Ham .    1.262875e-13\n",
      "Name: 293, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 1, Cluster 8:\")\n",
    "show_top_factors(comp_18, cluster_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 2, Cluster 7:\n",
      "\n",
      "Component 0:\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Component 205:\n",
      "Ham .    1.406077e-13\n",
      "Ham .    1.397978e-13\n",
      "Ham .    1.391472e-13\n",
      "Ham .    1.391472e-13\n",
      "Ham .    1.391472e-13\n",
      "Name: 205, dtype: float64\n",
      "\n",
      "Component 293:\n",
      "Ham .    1.326131e-13\n",
      "Ham .    1.295566e-13\n",
      "Ham .    1.262961e-13\n",
      "Ham .    1.262875e-13\n",
      "Ham .    1.262875e-13\n",
      "Name: 293, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 2, Cluster 7:\")\n",
    "show_top_factors(comp_27, cluster_27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 3, Cluster 3:\n",
      "\n",
      "Component 0:\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Component 205:\n",
      "Ham .    1.406077e-13\n",
      "Ham .    1.397978e-13\n",
      "Ham .    1.391472e-13\n",
      "Ham .    1.391472e-13\n",
      "Ham .    1.391472e-13\n",
      "Name: 205, dtype: float64\n",
      "\n",
      "Component 293:\n",
      "Ham .    1.326131e-13\n",
      "Ham .    1.295566e-13\n",
      "Ham .    1.262961e-13\n",
      "Ham .    1.262875e-13\n",
      "Ham .    1.262875e-13\n",
      "Name: 293, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 3, Cluster 3:\")\n",
    "show_top_factors(comp_33, cluster_33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cluster appears to be entirely made up of sentences where Hamlet is about to say a line. \n",
    "\n",
    "Component 0, the most important by far for each component simply seems to model the line prompt for Hamlet from _Hamlet_ (in this version of the play, lines are prompted by the first 3-4 letters of the character's name and a period). \n",
    "\n",
    "All the other component coefficients are extremely low for each entry, which is constantly Hamlet line prompts. These coeffecients are entirely unrelated, and impossible to discern from this alone.\n",
    "\n",
    "This cluster is obviously just Hamlet line prompts, pure and simple. It is interesting that each KMeans iteration found this to be the most important cluster to create over and over. Probably because line prompts are extremely predictable and easy to model and Hamlet has a lot of lines in _Hamlet_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (9, 5, 0): Bryant, Carroll, and Shakespeare\n",
    "\n",
    "Though not comletely the same between runs, this cluster was still relatively remarkably similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 1, Cluster 9:\n",
      "\n",
      "Component 4:\n",
      "King .    0.969387\n",
      "King .    0.969387\n",
      "King .    0.969387\n",
      "King .    0.969387\n",
      "King .    0.969387\n",
      "Name: 4, dtype: float64\n",
      "\n",
      "Component 24:\n",
      "' It ' s the oldest rule in the book ,' said the King .                                                                                                  0.527404\n",
      "Aloft , like a royal czar and king , the sun seemed giving this gentle air to this bold and rolling sea ; even as bride to groom .                       0.057380\n",
      "At the end of three years , the Pope called the Emperor of Allemaine and the King of Sicily , his brothers , to a great meeting in his city of Rome .    0.054680\n",
      "And that same night a mighty sea - king came up and slew Frode and plundered his city .                                                                  0.051369\n",
      "At last all the festivities were over , and the King of Sicily went home to his own land again , with his people .                                       0.045215\n",
      "Name: 24, dtype: float64\n",
      "\n",
      "Component 35:\n",
      "' Don ' t be impertinent ,' said the King , ' and don ' t look at me like that !'                           0.514241\n",
      "' I don ' t like the look of it at all ,' said the King : ' however , it may kiss my hand if it likes .'    0.312933\n",
      "' Then the words don ' t FIT you ,' said the King , looking round the court with a smile .                  0.262432\n",
      "' A cat may look at a king ,' said Alice .                                                                  0.069632\n",
      "\" I ' m going to Dublin , to try to make a court for the king .\"                                            0.042635\n",
      "Name: 35, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 1, Cluster 9:\")\n",
    "show_top_factors(comp_19, cluster_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 2, Cluster 5:\n",
      "\n",
      "Component 4:\n",
      "King .    0.969387\n",
      "King .    0.969387\n",
      "King .    0.969387\n",
      "King .    0.969387\n",
      "King .    0.969387\n",
      "Name: 4, dtype: float64\n",
      "\n",
      "Component 24:\n",
      "' It ' s the oldest rule in the book ,' said the King .                                                                                                  0.527404\n",
      "Aloft , like a royal czar and king , the sun seemed giving this gentle air to this bold and rolling sea ; even as bride to groom .                       0.057380\n",
      "At the end of three years , the Pope called the Emperor of Allemaine and the King of Sicily , his brothers , to a great meeting in his city of Rome .    0.054680\n",
      "And that same night a mighty sea - king came up and slew Frode and plundered his city .                                                                  0.051369\n",
      "At last all the festivities were over , and the King of Sicily went home to his own land again , with his people .                                       0.045215\n",
      "Name: 24, dtype: float64\n",
      "\n",
      "Component 35:\n",
      "' Don ' t be impertinent ,' said the King , ' and don ' t look at me like that !'                           0.514241\n",
      "' I don ' t like the look of it at all ,' said the King : ' however , it may kiss my hand if it likes .'    0.312933\n",
      "' Then the words don ' t FIT you ,' said the King , looking round the court with a smile .                  0.262432\n",
      "' A cat may look at a king ,' said Alice .                                                                  0.069632\n",
      "\" I ' m going to Dublin , to try to make a court for the king .\"                                            0.042635\n",
      "Name: 35, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 2, Cluster 5:\")\n",
    "show_top_factors(comp_25, cluster_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 3, Cluster 0:\n",
      "\n",
      "Component 4:\n",
      "King .    0.969387\n",
      "King .    0.969387\n",
      "King .    0.969387\n",
      "King .    0.969387\n",
      "King .    0.969387\n",
      "Name: 4, dtype: float64\n",
      "\n",
      "Component 24:\n",
      "' It ' s the oldest rule in the book ,' said the King .                                                                                                  0.527404\n",
      "Aloft , like a royal czar and king , the sun seemed giving this gentle air to this bold and rolling sea ; even as bride to groom .                       0.057380\n",
      "At the end of three years , the Pope called the Emperor of Allemaine and the King of Sicily , his brothers , to a great meeting in his city of Rome .    0.054680\n",
      "And that same night a mighty sea - king came up and slew Frode and plundered his city .                                                                  0.051369\n",
      "At last all the festivities were over , and the King of Sicily went home to his own land again , with his people .                                       0.045215\n",
      "Name: 24, dtype: float64\n",
      "\n",
      "Component 35:\n",
      "' Don ' t be impertinent ,' said the King , ' and don ' t look at me like that !'                           0.514241\n",
      "' I don ' t like the look of it at all ,' said the King : ' however , it may kiss my hand if it likes .'    0.312933\n",
      "' Then the words don ' t FIT you ,' said the King , looking round the court with a smile .                  0.262432\n",
      "' A cat may look at a king ,' said Alice .                                                                  0.069632\n",
      "\" I ' m going to Dublin , to try to make a court for the king .\"                                            0.042635\n",
      "Name: 35, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 3, Cluster 0:\")\n",
    "show_top_factors(comp_30, cluster_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cluster shares all the same top components: 4, 24, and 35. \n",
    "\n",
    "Component 4 deals wih the line prompt for the King in Hamlet.\n",
    "\n",
    "Component 24 deals with sentences about Kings and cities.\n",
    "\n",
    "Component 35 deals with quotoations talking about or said by a king.\n",
    "\n",
    "The obvious theme for this cluster is Kings, dealing with every aspect of the 3 texts which include kings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6, 9, 9): Melville and Austin\n",
    "This cluster was extremely similar in count between KMeans runs, though not exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 1, Cluster6 6:\n",
      "\n",
      "Component 1:\n",
      "CHAPTER 64     0.999719\n",
      "Chapter 4      0.999719\n",
      "CHAPTER 106    0.999719\n",
      "CHAPTER 69     0.999719\n",
      "CHAPTER 30     0.999719\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "Component 137:\n",
      "CHAPTER II       0.316544\n",
      "CHAPTER II .     0.316544\n",
      "CHAPTER III .    0.059989\n",
      "CHAPTER III .    0.059989\n",
      "CHAPTER III      0.059989\n",
      "Name: 137, dtype: float64\n",
      "\n",
      "Component 226:\n",
      "CHAPTER III                                                                  0.623005\n",
      "CHAPTER III .                                                                0.623005\n",
      "CHAPTER III .                                                                0.623005\n",
      "CHAPTER IV .                                                                 0.017074\n",
      "* Vide \" Priestley ' s History of Vision ,\" chapter on coloured shadows .    0.001236\n",
      "Name: 226, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 1, Cluster6 6:\")\n",
    "show_top_factors(comp_16, cluster_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 2, Cluster 9:\n",
      "\n",
      "Component 1:\n",
      "CHAPTER 64     0.999719\n",
      "Chapter 4      0.999719\n",
      "CHAPTER 106    0.999719\n",
      "CHAPTER 69     0.999719\n",
      "CHAPTER 30     0.999719\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "Component 137:\n",
      "CHAPTER II       0.316544\n",
      "CHAPTER II .     0.316544\n",
      "CHAPTER III .    0.059989\n",
      "CHAPTER III .    0.059989\n",
      "CHAPTER III      0.059989\n",
      "Name: 137, dtype: float64\n",
      "\n",
      "Component 226:\n",
      "CHAPTER III                                                                  0.623005\n",
      "CHAPTER III .                                                                0.623005\n",
      "CHAPTER III .                                                                0.623005\n",
      "CHAPTER IV .                                                                 0.017074\n",
      "* Vide \" Priestley ' s History of Vision ,\" chapter on coloured shadows .    0.001236\n",
      "Name: 226, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 2, Cluster 9:\")\n",
    "show_top_factors(comp_29, cluster_29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 3, Cluster 9:\n",
      "\n",
      "Component 1:\n",
      "CHAPTER 64     0.999719\n",
      "Chapter 4      0.999719\n",
      "CHAPTER 106    0.999719\n",
      "CHAPTER 127    0.999719\n",
      "CHAPTER 69     0.999719\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "Component 137:\n",
      "CHAPTER II       0.316544\n",
      "CHAPTER II .     0.316544\n",
      "CHAPTER III .    0.059989\n",
      "CHAPTER III .    0.059989\n",
      "CHAPTER III      0.059989\n",
      "Name: 137, dtype: float64\n",
      "\n",
      "Component 226:\n",
      "CHAPTER III                                                                  0.623005\n",
      "CHAPTER III .                                                                0.623005\n",
      "CHAPTER III .                                                                0.623005\n",
      "CHAPTER IV .                                                                 0.017074\n",
      "* Vide \" Priestley ' s History of Vision ,\" chapter on coloured shadows .    0.001236\n",
      "Name: 226, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 3, Cluster 9:\")\n",
    "show_top_factors(comp_39, cluster_39)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusters here share the same top 3 components, a testiment to their similarity: 1, 137, and 226.\n",
    "\n",
    "Component 1 deals with Chapter Titles ending in numbers.\n",
    "\n",
    "Component 137 likely deals with something unrelated to chapters titles judging by the low component correlation of each entry.  This indicates this cluster mostly or only contains chapter titles.\n",
    "\n",
    "Component 226 seems to deal with Chapter Titles with roman numerals.\n",
    "\n",
    "This cluster seems to contain Chapter titles, and perhaps other sentence fragments here and there with the word 'chapter' judging by the fact that is isn't perfectly the same between iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __(5, 8, 8)__: High Pop, Edgeworth Dominated\n",
    "This is the most populated cluster by far. It probably has a common theme that all authors share and the fact that it is mostly Edgeworth comes from the fact that Edgeworth simply has more data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 1, Cluster 5:\n",
      "\n",
      "Component 22:\n",
      "Ophe .    1.0\n",
      "Ophe .    1.0\n",
      "Ophe .    1.0\n",
      "Ophe .    1.0\n",
      "Ophe .    1.0\n",
      "Name: 22, dtype: float64\n",
      "\n",
      "Component 26:\n",
      "Mar .    1.0\n",
      "Mar .    1.0\n",
      "Mar .    1.0\n",
      "Mar .    1.0\n",
      "Mar .    1.0\n",
      "Name: 26, dtype: float64\n",
      "\n",
      "Component 37:\n",
      "Pol .    1.0\n",
      "Pol .    1.0\n",
      "Pol .    1.0\n",
      "Pol .    1.0\n",
      "Pol .    1.0\n",
      "Name: 37, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 1, Cluster 5:\")\n",
    "show_top_factors(comp_15, cluster_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 2, Cluster 8:\n",
      "\n",
      "Component 14:\n",
      "Qu .    0.999992\n",
      "Qu .    0.999992\n",
      "Qu .    0.999992\n",
      "Qu .    0.999992\n",
      "Qu .    0.999992\n",
      "Name: 14, dtype: float64\n",
      "\n",
      "Component 37:\n",
      "Pol .    1.0\n",
      "Pol .    1.0\n",
      "Pol .    1.0\n",
      "Pol .    1.0\n",
      "Pol .    1.0\n",
      "Name: 37, dtype: float64\n",
      "\n",
      "Component 91:\n",
      "Osr .    1.0\n",
      "Osr .    1.0\n",
      "Osr .    1.0\n",
      "Osr .    1.0\n",
      "Osr .    1.0\n",
      "Name: 91, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 2, Cluster 8:\")\n",
    "show_top_factors(comp_28, cluster_28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 3, Cluster 8:\n",
      "\n",
      "Component 22:\n",
      "Ophe .    1.0\n",
      "Ophe .    1.0\n",
      "Ophe .    1.0\n",
      "Ophe .    1.0\n",
      "Ophe .    1.0\n",
      "Name: 22, dtype: float64\n",
      "\n",
      "Component 26:\n",
      "Mar .    1.0\n",
      "Mar .    1.0\n",
      "Mar .    1.0\n",
      "Mar .    1.0\n",
      "Mar .    1.0\n",
      "Name: 26, dtype: float64\n",
      "\n",
      "Component 37:\n",
      "Pol .    1.0\n",
      "Pol .    1.0\n",
      "Pol .    1.0\n",
      "Pol .    1.0\n",
      "Pol .    1.0\n",
      "Name: 37, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 3, Cluster 8:\")\n",
    "show_top_factors(comp_38, cluster_38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important factors to this cluster group were all different, but they were all Shakespeare play line prompts. This is interesting since Shakespeare is a low minority in this cluster.\n",
    "\n",
    "This is explained by the fact that line prompts are a very predictable concept, so they will be an important factor to any cluster they are a part of. Because so many different authors are contained in this group, in fact near half of the text is contained in this grouped, it must model something more common than line prompts. \n",
    "\n",
    "This cluster likely models short sentences, which line prompts would all fall into and pretty much all texts have a lot of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2, 0, 1): Byant and the Rest\n",
    "This cluster is populated by a lot of authors, but the Bryant domination is clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 1, Cluster 2:\n",
      "\n",
      "Component 3:\n",
      "\" Oh , no , no ; you are too little !\"                            0.662676\n",
      "\" Oh , such a little creature ; to have so much sense , too !\"    0.572320\n",
      "\" Oh no , no , no ; you are too little , you are too little !\"    0.435618\n",
      "\" Oh , sing again , little Nightingale ,\" begged Death .          0.347084\n",
      "\" Oh ,\" said the little Jackal , \" you want my opinion ?          0.254034\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "Component 15:\n",
      "\" A little .                   0.535618\n",
      "THE LITTLE MERCHANTS .         0.529686\n",
      "THE LITTLE VAGABOND            0.528746\n",
      "TWO LITTLE RIDDLES IN RHYME    0.528551\n",
      "LITTLE JACK ROLLAROUND         0.523913\n",
      "Name: 15, dtype: float64\n",
      "\n",
      "Component 38:\n",
      "Once upon a time there was a little Red Hen , who lived on a farm all by herself .                                                                                                                                                                                                                                                                                                            0.564709\n",
      "Syme was able to pour out for the first time the whole of his outrageous tale , from the time when Gregory had taken him to the little tavern by the river .                                                                                                                                                                                                                                  0.033704\n",
      "More than seven years were gone since this little history of sorrowful interest had reached its close ; and time had softened down much , perhaps nearly all of peculiar attachment to him , but she had been too dependent on time alone ; no aid had been given in change of place ( except in one visit to Bath soon after the rupture ), or in any novelty or enlargement of society .    0.030960\n",
      "ANOTHER LITTLE RED HEN                                                                                                                                                                                                                                                                                                                                                                        0.026819\n",
      "\" I will , then ,\" said the little Red Hen .                                                                                                                                                                                                                                                                                                                                                  0.025935\n",
      "Name: 38, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 1, Cluster 2:\")\n",
    "show_top_factors(comp_12, cluster_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 2, Cluster 0:\n",
      "\n",
      "Component 15:\n",
      "\" A little .                   0.535618\n",
      "THE LITTLE MERCHANTS .         0.529686\n",
      "THE LITTLE VAGABOND            0.528746\n",
      "TWO LITTLE RIDDLES IN RHYME    0.528551\n",
      "LITTLE JACK ROLLAROUND         0.523913\n",
      "Name: 15, dtype: float64\n",
      "\n",
      "Component 32:\n",
      "\" You dear , lovely little thing !\"                                                                                                                                                                                    0.506123\n",
      "\" DEAR MARY , NANCY , AND LITTLE PEG ,                                                                                                                                                                                 0.452717\n",
      "Once upon a time there was a dear little girl , whose name was Elsa .                                                                                                                                                  0.396748\n",
      "The old Alligator was so furious that he crawled up on the bank and went after the little Jackal ; but , dear , dear , he couldn ' t catch the little Jackal ; he ran far too fast .                                   0.368989\n",
      "Grieve not so , dear mother , ( the just - grown daughter speaks through her sobs , The little sisters huddle around speechless and dismay ' d ,) See , dearest mother , the letter says Pete will soon be better .    0.346268\n",
      "Name: 32, dtype: float64\n",
      "\n",
      "Component 38:\n",
      "Once upon a time there was a little Red Hen , who lived on a farm all by herself .                                                                                                                                                                                                                                                                                                            0.564709\n",
      "Syme was able to pour out for the first time the whole of his outrageous tale , from the time when Gregory had taken him to the little tavern by the river .                                                                                                                                                                                                                                  0.033704\n",
      "More than seven years were gone since this little history of sorrowful interest had reached its close ; and time had softened down much , perhaps nearly all of peculiar attachment to him , but she had been too dependent on time alone ; no aid had been given in change of place ( except in one visit to Bath soon after the rupture ), or in any novelty or enlargement of society .    0.030960\n",
      "ANOTHER LITTLE RED HEN                                                                                                                                                                                                                                                                                                                                                                        0.026819\n",
      "\" I will , then ,\" said the little Red Hen .                                                                                                                                                                                                                                                                                                                                                  0.025935\n",
      "Name: 38, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 2, Cluster 0:\")\n",
    "show_top_factors(comp_20, cluster_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 3, Cluster 1:\n",
      "\n",
      "Component 3:\n",
      "\" Oh , no , no ; you are too little !\"                            0.662676\n",
      "\" Oh , such a little creature ; to have so much sense , too !\"    0.572320\n",
      "\" Oh no , no , no ; you are too little , you are too little !\"    0.435618\n",
      "\" Oh , sing again , little Nightingale ,\" begged Death .          0.347084\n",
      "\" Oh ,\" said the little Jackal , \" you want my opinion ?          0.254034\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "Component 15:\n",
      "\" A little .                   0.535618\n",
      "THE LITTLE MERCHANTS .         0.529686\n",
      "THE LITTLE VAGABOND            0.528746\n",
      "TWO LITTLE RIDDLES IN RHYME    0.528551\n",
      "LITTLE JACK ROLLAROUND         0.523913\n",
      "Name: 15, dtype: float64\n",
      "\n",
      "Component 38:\n",
      "Once upon a time there was a little Red Hen , who lived on a farm all by herself .                                                                                                                                                                                                                                                                                                            0.564709\n",
      "Syme was able to pour out for the first time the whole of his outrageous tale , from the time when Gregory had taken him to the little tavern by the river .                                                                                                                                                                                                                                  0.033704\n",
      "More than seven years were gone since this little history of sorrowful interest had reached its close ; and time had softened down much , perhaps nearly all of peculiar attachment to him , but she had been too dependent on time alone ; no aid had been given in change of place ( except in one visit to Bath soon after the rupture ), or in any novelty or enlargement of society .    0.030960\n",
      "ANOTHER LITTLE RED HEN                                                                                                                                                                                                                                                                                                                                                                        0.026819\n",
      "\" I will , then ,\" said the little Red Hen .                                                                                                                                                                                                                                                                                                                                                  0.025935\n",
      "Name: 38, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 3, Cluster 1:\")\n",
    "show_top_factors(comp_31, cluster_31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all clusters share their top 3 components here, only 2 are shared: 15 and 38. The First and Third iteration share component 3 and the Second iteration has component 32.\n",
    "\n",
    "Component 15 seems to deal with titles or other very short sentences using the word 'little'.\n",
    "\n",
    "Component 38 deals with sentences with more than one clause which use the word 'little'.\n",
    "\n",
    "Component 3 deals with dialouge where one character describes another as 'little' and uses the word 'oh'.\n",
    "\n",
    "Component 32 deals with descriptive sentences where a character is decribed as little and somehow alone or unreachable by other characters.\n",
    "\n",
    "The theme of this cluster is very clear, if not a little odd, no pun intended. It completely deals with sentences using the word 'little'. If I wanted to get rid of this cluster I might add 'little' to the stop words for this vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4, 2, 6): Edgeworth, Chesterton, and Carroll, minus Blake, Burgess, and Skakespeare.\n",
    "This cluster has many authors, but the fact that is never has Blake, Burgess, or Shakespeare may be a key to it's theme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 1, Cluster 4:\n",
      "\n",
      "Component 2:\n",
      "\" Oh , at Harrow ,\" said the policeman                                                         0.735348\n",
      "\" Now you do that ,\" said he .                                                                 0.671664\n",
      "\" I have done it ,\" he said hoarsely .                                                         0.662345\n",
      "Whoever degrades another degrades me , And whatever is done or said returns at last to me .    0.655795\n",
      "\" They have done us ,\" he said , with brief military irony .                                   0.648707\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "Component 6:\n",
      "\" Yes , sir ,\" said Sir Arthur , pulling the lease out of his pocket .                                                                                           0.702896\n",
      "\" Yes , sir , everything ,\" said the attendant gravely .                                                                                                         0.594378\n",
      "\" You have done him no injury ,\" said Sir Arthur , coolly .                                                                                                      0.574710\n",
      "\" I have a letter for Mrs . Churchill , sir ,\" said Franklin , endeavouring to pronounce his \" sir \" in a tone as respectful as the butler ' s was insolent .    0.569482\n",
      "\" Let us go and see this nook ,\" said Sir Arthur .                                                                                                               0.503212\n",
      "Name: 6, dtype: float64\n",
      "\n",
      "Component 35:\n",
      "\" No ; but you don ' t TAKE me ,\" said Fisher ; \" you don ' t TAKE me .                                   0.597199\n",
      "\" Don ' t you think , then ,\" he said in a dangerous voice , \" that I am serious about my anarchism ?\"    0.567719\n",
      "\" You don ' t expect me ,\" he said , \" to revolutionise society on this lawn ?\"                           0.534743\n",
      "' No , I didn ' t ,' said Alice : ' I don ' t think it ' s at all a pity .                                0.509696\n",
      "\" You don ' t think he ' s such a fool , when he can get himself off ,\" said Tarlton .                    0.483334\n",
      "Name: 35, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 1, Cluster 4:\")\n",
    "show_top_factors(comp_14, cluster_14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 2, Cluster 2:\n",
      "\n",
      "Component 2:\n",
      "\" Now you do that ,\" said he .                                                                 0.671664\n",
      "\" I have done it ,\" he said hoarsely .                                                         0.662345\n",
      "Whoever degrades another degrades me , And whatever is done or said returns at last to me .    0.655795\n",
      "\" They have done us ,\" he said , with brief military irony .                                   0.648707\n",
      "\" It is jolly to get some pals ,\" he said .                                                    0.643445\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "Component 6:\n",
      "\" Yes , sir ,\" said Sir Arthur , pulling the lease out of his pocket .                                                                                           0.702896\n",
      "\" Yes , sir , everything ,\" said the attendant gravely .                                                                                                         0.594378\n",
      "\" You have done him no injury ,\" said Sir Arthur , coolly .                                                                                                      0.574710\n",
      "\" I have a letter for Mrs . Churchill , sir ,\" said Franklin , endeavouring to pronounce his \" sir \" in a tone as respectful as the butler ' s was insolent .    0.569482\n",
      "\" Let us go and see this nook ,\" said Sir Arthur .                                                                                                               0.503212\n",
      "Name: 6, dtype: float64\n",
      "\n",
      "Component 35:\n",
      "\" No ; but you don ' t TAKE me ,\" said Fisher ; \" you don ' t TAKE me .                                   0.597199\n",
      "\" Don ' t you think , then ,\" he said in a dangerous voice , \" that I am serious about my anarchism ?\"    0.567719\n",
      "\" You don ' t expect me ,\" he said , \" to revolutionise society on this lawn ?\"                           0.534743\n",
      "' No , I didn ' t ,' said Alice : ' I don ' t think it ' s at all a pity .                                0.509696\n",
      "\" You don ' t think he ' s such a fool , when he can get himself off ,\" said Tarlton .                    0.483334\n",
      "Name: 35, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 2, Cluster 2:\")\n",
    "show_top_factors(comp_22, cluster_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 3, Cluster 6:\n",
      "\n",
      "Component 2:\n",
      "\" Oh , at Harrow ,\" said the policeman                                                         0.735348\n",
      "\" Now you do that ,\" said he .                                                                 0.671664\n",
      "\" I have done it ,\" he said hoarsely .                                                         0.662345\n",
      "Whoever degrades another degrades me , And whatever is done or said returns at last to me .    0.655795\n",
      "\" They have done us ,\" he said , with brief military irony .                                   0.648707\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "Component 3:\n",
      "\" Oh !\"               0.873917\n",
      "\" Oh !                0.873917\n",
      "\" Oh !                0.873917\n",
      "\" Oh , EDICATION !    0.873917\n",
      "\" Oh !                0.873917\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "Component 6:\n",
      "\" Yes , sir ,\" said Sir Arthur , pulling the lease out of his pocket .                                                                                           0.702896\n",
      "\" Yes , sir , everything ,\" said the attendant gravely .                                                                                                         0.594378\n",
      "\" You have done him no injury ,\" said Sir Arthur , coolly .                                                                                                      0.574710\n",
      "\" I have a letter for Mrs . Churchill , sir ,\" said Franklin , endeavouring to pronounce his \" sir \" in a tone as respectful as the butler ' s was insolent .    0.569482\n",
      "\" Let us go and see this nook ,\" said Sir Arthur .                                                                                                               0.503212\n",
      "Name: 6, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 3, Cluster 6:\")\n",
    "show_top_factors(comp_36, cluster_36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cluster also does not share all it's top components, only having 2 i all 3 iterations: components 2 and 6. The First and Second iteration share top component 35 and the Third has component 3. \n",
    "\n",
    "Component 2 deals with things being done to others, especially with the connotation of authority.\n",
    "\n",
    "Component 6 deals with quotations using the word 'sir' and/or adressing a character with 'Sir' in their name.\n",
    "\n",
    "Component 35 seems to deal with quotations where one character expresses some kind of will to not do something. \n",
    "\n",
    "Component 3 deals with exclaimations using 'Oh'.\n",
    "\n",
    "This cluster seems to have a theme of quotation responding to authority, whether it be by complying or refusing to comply. The authors omitted make perfect sense because they do not use quotations in their writing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7, 1, 4): Melville, Edgeworth, and Chesterton minus Blake, Burgess, and Carroll.\n",
    "Once again, remebering which authors are omitted may be key to finding the theme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 1, Cluster 7:\n",
      "\n",
      "Component 12:\n",
      "\" You don ' t know , man !                                                                                         0.647269\n",
      "\" But if you don ' t teach him better now he is a child , how will he know when he is a man ?\"                     0.459112\n",
      "\" Did you know ,\" he asked , \" that that man Gogol was one of us ?\"                                                0.430968\n",
      "\" I know it , old man ; these stubbs will weld together like glue from the melted bones of murderers .             0.336463\n",
      "\" I did not know that I fixed my eyes upon you ; I was thinking of my fireworks ,\" said the poor man , simply .    0.302457\n",
      "Name: 12, dtype: float64\n",
      "\n",
      "Component 14:\n",
      "Qu .    0.999992\n",
      "Qu .    0.999992\n",
      "Qu .    0.999992\n",
      "Qu .    0.999992\n",
      "Qu .    0.999992\n",
      "Name: 14, dtype: float64\n",
      "\n",
      "Component 17:\n",
      "\" What ' s the old man have so much to do with him for ?\"    0.585315\n",
      "Old Man .                                                    0.585315\n",
      "\" Have you not robbed this old man ?                         0.579286\n",
      "\" Well , man !                                               0.566657\n",
      "It is , man .                                                0.566657\n",
      "Name: 17, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 1, Cluster 7:\")\n",
    "show_top_factors(comp_17, cluster_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 2, Cluster 1:\n",
      "\n",
      "Component 11:\n",
      "Laer .    1.0\n",
      "Laer .    1.0\n",
      "Laer .    1.0\n",
      "Laer .    1.0\n",
      "Laer .    1.0\n",
      "Name: 11, dtype: float64\n",
      "\n",
      "Component 17:\n",
      "Old Man .                                                    0.585315\n",
      "\" What ' s the old man have so much to do with him for ?\"    0.585315\n",
      "\" Have you not robbed this old man ?                         0.579286\n",
      "\" Well , man !                                               0.566657\n",
      "It is , man .                                                0.566657\n",
      "Name: 17, dtype: float64\n",
      "\n",
      "Component 22:\n",
      "Ophe .    1.0\n",
      "Ophe .    1.0\n",
      "Ophe .    1.0\n",
      "Ophe .    1.0\n",
      "Ophe .    1.0\n",
      "Name: 22, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 2, Cluster 1:\")\n",
    "show_top_factors(comp_21, cluster_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 3, Cluster 4:\n",
      "\n",
      "Component 2:\n",
      "\" Oh , they have been punished enough ,\" said the old man ; \" forgive them , sir .\"     0.571382\n",
      "\" Oh , shut it ,\" said the man in spectacles .                                          0.527614\n",
      "\" I am ,\" said the man .                                                                0.471594\n",
      "\" I found ,\" said he , \" that I was considered by Harville an engaged man !             0.422711\n",
      "\" Don ' t be such a silly man ,\" he said , with the effeminate dignity of a curate .    0.409533\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "Component 12:\n",
      "\" You don ' t know , man !                                                                                0.647269\n",
      "\" But if you don ' t teach him better now he is a child , how will he know when he is a man ?\"            0.459112\n",
      "\" Did you know ,\" he asked , \" that that man Gogol was one of us ?\"                                       0.430968\n",
      "\" I don ' t know that , my little man ; I never yet saw him kneel .\"                                      0.385355\n",
      "\" I know it , old man ; these stubbs will weld together like glue from the melted bones of murderers .    0.336463\n",
      "Name: 12, dtype: float64\n",
      "\n",
      "Component 17:\n",
      "\" What ' s the old man have so much to do with him for ?\"    0.585315\n",
      "Old Man .                                                    0.585315\n",
      "\" Have you not robbed this old man ?                         0.579286\n",
      "\" Well , man !                                               0.566657\n",
      "It is , man .                                                0.566657\n",
      "Name: 17, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 3, Cluster 4:\")\n",
    "show_top_factors(comp_34, cluster_34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cluster was markedly less recognizeable than the others, and it shows in its top factors across iterations, only sharing 1 top component, 17. 12 was also shared by the First and Third iterations, but the rest of the top compenents, 2, 11, 12, 22, were unique.  Remember, eahc iteration undoubtedly values these compenents highly so they will all be usefull to find the theme; they just don't value them in exactly the same way.\n",
    "\n",
    "Component 17 deals with quotations and short sentences address a man or containing the word 'man', especially and old man.\n",
    "\n",
    "Component 12 deals with quotations about men, especially concerning their membership of a particular group. \n",
    "\n",
    "Component 2 deals with quotation about men, especially about one charter acknowledging another character. \n",
    "\n",
    "Component 14, which is only is the First iteration, deals with line prompts from Qu from _Hamlet_. So the first iteration got line prompts from one character.\n",
    "\n",
    "Component 11 and 22, which are only top components in the Second iteration, deal with line prompts from 2 particular characters from _Hamlet_. So the second iteration got line prompts from 2 characters, indicating the overall theme may be weakest in it.\n",
    "\n",
    "This cluster seems to deal with sentences and quotations dealing with the word 'man'. Though it is somewhat inconsistent, and while frequentll get line prompts, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1, 3, 7): Whitman and Melville\n",
    "The least consistent of all the recognizable clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 1, Cluster 1:\n",
      "\n",
      "Component 7:\n",
      "Hor .    1.0\n",
      "Hor .    1.0\n",
      "Hor .    1.0\n",
      "Hor .    1.0\n",
      "Hor .    1.0\n",
      "Name: 7, dtype: float64\n",
      "\n",
      "Component 8:\n",
      "Just .    0.848219\n",
      "Just .    0.848219\n",
      "Just .    0.848219\n",
      "Just .    0.848219\n",
      "Just .    0.848219\n",
      "Name: 8, dtype: float64\n",
      "\n",
      "Component 13:\n",
      "\" Great Lord !\"                                                                                                                                                                                                                       0.777441\n",
      "\" Now the Lord had prepared a great fish to swallow up Jonah .\"                                                                                                                                                                       0.596483\n",
      "When serenely advancing on one of these journeys , if any strange suspicious sights are seen , my lord whale keeps a wary eye on his interesting family .                                                                             0.471746\n",
      "\"' I am a Hebrew ,' he cries  and then ' I fear the Lord the God of Heaven who hath made the sea and the dry land !'                                                                                                                  0.446523\n",
      "But David looked at him , and answered , \" Thou comest to me with a sword , and with a spear , and with a shield ; but I come to thee in the name of the Lord of hosts , the God of the armies of Israel , whom thou hast defied .    0.322317\n",
      "Name: 13, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 1, Cluster 1:\")\n",
    "show_top_factors(comp_11, cluster_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 2, Cluster 3:\n",
      "\n",
      "Component 7:\n",
      "Hor .    1.0\n",
      "Hor .    1.0\n",
      "Hor .    1.0\n",
      "Hor .    1.0\n",
      "Hor .    1.0\n",
      "Name: 7, dtype: float64\n",
      "\n",
      "Component 23:\n",
      "\" And when shall I find it ?\"                                                                                              0.764985\n",
      "6 What will be will be well , for what is is well , To take interest is well , and not to take interest shall be well .    0.764985\n",
      "\" Where shall we go ?\"                                                                                                     0.764985\n",
      "\" Perhaps I shall .                                                                                                        0.764985\n",
      "\"' Shall we ?'                                                                                                             0.764985\n",
      "Name: 23, dtype: float64\n",
      "\n",
      "Component 26:\n",
      "Mar .    1.0\n",
      "Mar .    1.0\n",
      "Mar .    1.0\n",
      "Mar .    1.0\n",
      "Mar .    1.0\n",
      "Name: 26, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 2, Cluster 3:\")\n",
    "show_top_factors(comp_23, cluster_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Factors for K-Means 3, Cluster 7:\n",
      "\n",
      "Component 23:\n",
      "\" And when shall I find it ?\"                                                                                              0.764985\n",
      "\"' Shall we ?'                                                                                                             0.764985\n",
      "\" Shall we go back , then ?\"                                                                                               0.764985\n",
      "6 What will be will be well , for what is is well , To take interest is well , and not to take interest shall be well .    0.764985\n",
      "\" Where shall we go ?\"                                                                                                     0.764985\n",
      "Name: 23, dtype: float64\n",
      "\n",
      "Component 24:\n",
      "[ BOOK XXV ]       0.804744\n",
      "[ BOOK XXXI ]      0.804744\n",
      "[ BOOK XXIX ]      0.804744\n",
      "[ BOOK XXVIII ]    0.804744\n",
      "[ BOOK XXXII .     0.804744\n",
      "Name: 24, dtype: float64\n",
      "\n",
      "Component 34:\n",
      "6 Land of lands and bards to corroborate !                                                                                                                                                                                                                                                                   0.668947\n",
      "Wherein differ the sea and the land , that a miracle upon one is not a miracle upon the other ?                                                                                                                                                                                                              0.581576\n",
      "The noble son on sinewy feet advancing , I saw , out of the land of prairies , land of Ohio ' s waters and of Indiana , To the rescue the stalwart giant hurry his plenteous offspring , Drest in blue , bearing their trusty rifles on their shoulders .                                                    0.538698\n",
      "12 Lo , body and soul  this land , My own Manhattan with spires , and the sparkling and hurrying tides , and the ships , The varied and ample land , the South and the North in the light , Ohio ' s shores and flashing Missouri , And ever the far - spreading prairies cover ' d with grass and corn .    0.467998\n",
      "This is the grass that grows wherever the land is and the water is , This the common air that bathes the globe .                                                                                                                                                                                             0.427920\n",
      "Name: 34, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Factors for K-Means 3, Cluster 7:\")\n",
    "show_top_factors(comp_37, cluster_37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The irregularity of this cluster is really starting to show, having no top 3 components shared by all 3 iterations. Component 7 is shared by the First and Second iteration and Component 23 is shared by the Second and Third iteration.\n",
    "\n",
    "Component 7 deals with Horatio line prompts.\n",
    "\n",
    "Component 23 deals with questions using the word 'shall'\n",
    "\n",
    "For the components only in the First iteration: Component 8 deals with Justine line prompts. Components 13 deals with quotations using the word 'lord', especially when using it to literally mean god.\n",
    "\n",
    "The only top component in the Second iteration, 26, deals with Mar line prompts. \n",
    "\n",
    "The components only in the Third iteration are component 24 and 34. Component 24 deals with Book titles. Componenent 34 deals decriptive language that decribes natures, especially referring to god or personifying nature.\n",
    "\n",
    "Though it has some inconsistencies, sometimes getting random line prompts and Chapter titles, the overaching theme of this cluster seems to be characterizing some kind of higher power, perhaps even asking it for something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Author Using Various Models\n",
    "\n",
    "I will now test my ability to predict author from text. There are many different types of models with many different usesm but I will try 3 different ones here: Logistic Regression, Grandient-Boosted Decision Trees, and Random Forest. \n",
    "\n",
    "I will check each models cross validation score to check the overall health of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words Feature Set\n",
    "X_bow = total_word_counts.drop(['text_author','text_sentence'],1)\n",
    "y_bow = total_word_counts.text_author\n",
    "Xtrain_bow, Xtest_bow, ytrain_bow, ytest_bow = train_test_split(X_bow,y_bow,test_size=0.5,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C value is  0.01\n",
      "Best C value is  0.1\n",
      "Best C value is  1\n",
      "Best C value is  10\n",
      "Best C value is  100\n",
      "Best C value is  200\n"
     ]
    }
   ],
   "source": [
    "constants = [.01,.1,1,10,100,200,300,400,500,700,1000]\n",
    "bestc = 0\n",
    "bestscore = 0\n",
    "for c in constants:\n",
    "    lr_lsa = LogisticRegression(C=c,random_state=42)\n",
    "    lr_lsa.fit(X_train_lsa, Y_train)\n",
    "    score = lr_lsa.score(X_test_lsa, Y_test)\n",
    "    if score > bestscore:\n",
    "        bestc = c\n",
    "        bestscore = score\n",
    "        print('Best C value is ',c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_lsa = LogisticRegression(C=bestc,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.809009981116806\n",
      "\n",
      "Test set score: 0.7098732128405719\n"
     ]
    }
   ],
   "source": [
    "lr_lsa.fit(X_train_lsa, Y_train)\n",
    "print('Training set score:', lr_lsa.score(X_train_lsa, Y_train))\n",
    "print('\\nTest set score:', lr_lsa.score(X_test_lsa, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_lsa_cv = cross_val_score(lr_lsa, X_test_lsa, Y_test, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Cross Validation\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.68322148, 0.7033557 , 0.70659489, 0.68506057, 0.72005384,\n",
       "       0.70350404, 0.68918919, 0.70135135, 0.70420624, 0.70380435])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean and Standard Error:\n",
      "0.7  +/-  0.021\n"
     ]
    }
   ],
   "source": [
    "print('Logistic Regression Cross Validation\\n')\n",
    "display(lr_lsa_cv)\n",
    "print('\\nMean and Standard Error:')\n",
    "print(round(lr_lsa_cv.mean(),3),' +/- ',round(lr_lsa_cv.std()*2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-Boosted Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.668869705961694\n",
      "\n",
      "Test set score: 0.5967089290531427\n"
     ]
    }
   ],
   "source": [
    "clf_lsa = GradientBoostingClassifier(n_estimators=50,max_depth=2,random_state=42)\n",
    "clf_lsa.fit(X_train_lsa, Y_train)\n",
    "print('Training set score:', clf_lsa.score(X_train_lsa, Y_train))\n",
    "print('\\nTest set score:', clf_lsa.score(X_test_lsa, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lsa_cv = cross_val_score(clf_lsa, X_test_lsa, Y_test, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient-Boosted Tree Cross Validation\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.5870881 , 0.60740741, 0.60755226, 0.57837838, 0.5862069 ])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean and Standard Error:\n",
      "0.593  +/-  0.024\n"
     ]
    }
   ],
   "source": [
    "print('Gradient-Boosted Tree Cross Validation\\n')\n",
    "display(clf_lsa_cv)\n",
    "print('\\nMean and Standard Error:')\n",
    "print(round(clf_lsa_cv.mean(),3),' +/- ',round(clf_lsa_cv.std()*2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9579174534664149\n",
      "\n",
      "Test set score: 0.6154572430536822\n"
     ]
    }
   ],
   "source": [
    "rfc_lsa = RandomForestClassifier(n_estimators=100,random_state=42)\n",
    "rfc_lsa.fit(X_train_lsa, Y_train)\n",
    "print('Training set score:', rfc_lsa.score(X_train_lsa, Y_train))\n",
    "print('\\nTest set score:', rfc_lsa.score(X_test_lsa, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_lsa_cv = cross_val_score(rfc_lsa, X_test_lsa, Y_test, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Cross Validation\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.58439812, 0.5986532 , 0.60687795, 0.59459459, 0.5821501 ])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean and Standard Error:\n",
      "0.593  +/-  0.018\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest Cross Validation\\n')\n",
    "display(rfc_lsa_cv)\n",
    "print('\\nMean and Standard Error:')\n",
    "print(round(rfc_lsa_cv.mean(),3),' +/- ',round(rfc_lsa_cv.std()*2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Conclusion\n",
    "\n",
    "The Boosted model preformed the worst by far, having the lowest training and testing scores and performing badly in corss validation.\n",
    "\n",
    "The Random Forest model did not perform as badly, but not that well either. It had a 91% training score, but only a 60% testing score, indicating some type of overfitting. Though its cross validation scores were not that bad, indicating that any overfitting was not severe.\n",
    "\n",
    "The Logistic Regression model performed the best, having an 81% training score and a 71% percent testing score. Additionally, it performed the best in cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLustering Holdout Group\n",
    "\n",
    "Now I want to return to my clusters, and see if they retain similar groupings in the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmt1 = KMeans(n_clusters=10, random_state=0)\n",
    "y_predt1 = kmt1.fit_predict(X_test_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmt2 = KMeans(n_clusters=10, random_state=42)\n",
    "y_predt2 = kmt2.fit_predict(X_test_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmt3 = KMeans(n_clusters=10, random_state=1337)\n",
    "y_predt3 = kmt3.fit_predict(X_test_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans 1 Inertia:  6588.094229689489\n",
      "\n",
      "Comparing the K-Means 1 clusters to the actual author groupings:\n",
      "\n",
      "col_0          0    1   2     3    4   5   6    7   8    9\n",
      "row_0                                                     \n",
      "Austen        97   20  23   262   38  10   0    0  19   27\n",
      "Blake         12   13   1   116    2   1   0    0   0    7\n",
      "Bryant        26  121   7   291   85  12  32    0   0   26\n",
      "Burgess        1   30   2    85    6   4   0    0   0    8\n",
      "Carroll       18   30  11   180   99   9  12    0   2   17\n",
      "Chesterton    32   12  13   359  174   9   0    0   5   28\n",
      "Edgeworth    127   70  34  1165  277  58   3    0  75   79\n",
      "Melville      89   21  85   838   50  17  10    0  44  241\n",
      "Shakespeare    0    0   0   283    0   0  49  134   0    2\n",
      "Whitman       91   18   0   904    5  10   1    0   0  240\n"
     ]
    }
   ],
   "source": [
    "print('KMeans 1 Inertia: ',km1.inertia_)\n",
    "print('\\nComparing the K-Means 1 clusters to the actual author groupings:\\n')\n",
    "print(pd.crosstab(Y_test,y_predt1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans 2 Inertia:  6572.738657087793\n",
      "\n",
      "Comparing the K-Means 2 clusters to the actual author groupings:\n",
      "\n",
      "col_0          0   1    2   3   4    5    6     7    8   9\n",
      "row_0                                                     \n",
      "Austen        30  15    0   9   0    0   45   377   18   2\n",
      "Blake         14   2    0   0   0    0    2    83   50   1\n",
      "Bryant       117  14    5   0  32    0   83   277   46  26\n",
      "Burgess       34   2    0   0   0    0    6    87    4   3\n",
      "Carroll       34   8    0   8  12    0  105   195   11   5\n",
      "Chesterton    17  11    0   6   0    0  176   370   37  15\n",
      "Edgeworth    120  56    0   6   3    0  313  1247   99  44\n",
      "Melville      30  20  187  66   8    0   52   758  222  52\n",
      "Shakespeare    0   0    0   0  49  134    0   281    4   0\n",
      "Whitman       20   0    2   0   1    0    3   504  685  54\n"
     ]
    }
   ],
   "source": [
    "print('KMeans 2 Inertia: ',km2.inertia_)\n",
    "print('\\nComparing the K-Means 2 clusters to the actual author groupings:\\n')\n",
    "print(pd.crosstab(Y_test,y_predt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans 3 Inertia:  6586.917000775612\n",
      "\n",
      "Comparing the K-Means 3 clusters to the actual author groupings:\n",
      "\n",
      "col_0          0   1    2    3    4   5   6   7    8     9\n",
      "row_0                                                     \n",
      "Austen         0  14   28    0   51  15  12   9   40   327\n",
      "Blake          0   4   14    0    9   2   2   0    2   119\n",
      "Bryant         5  11  121    0   14  14  59   0   77   299\n",
      "Burgess        0   3   34    0    4   2   0   0    6    87\n",
      "Carroll        0   8   33    0   15   8  14   8  104   188\n",
      "Chesterton     0  16   18    0   12  11  34   6  161   374\n",
      "Edgeworth      0  28  117    0  128  57  40   6  305  1207\n",
      "Melville     183  53   28    0  240  21  59  66   51   694\n",
      "Shakespeare    0   0    0  134   21   0  82   0    0   231\n",
      "Whitman        2  79   18    0  139   0  43   0    4   984\n"
     ]
    }
   ],
   "source": [
    "print('KMeans 3 Inertia: ',km3.inertia_)\n",
    "print('\\nComparing the K-Means 3 clusters to the actual author groupings:\\n')\n",
    "print(pd.crosstab(Y_test,y_predt3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Clusters Conclusion\n",
    "\n",
    "The perfectly consistent Hamlet line prompt Shakespeare cluster makes an appearance. There is the super consistent king Bryant Carroll cluster. I can also see the authority Edgeworth-Chesterton-Carroll cluster. The 'little' Bryant dominated cluster is there as well. There is the commonly populated short-sentence cluster. The man cluster with low amounts of Blake, Burgess and Carroll looks to be there as well. I can also see the sparsely populated 'chapter' cluster with mostly Melville and Austin. There is even the inconsistence higher power Whitman and Melville cluster.\n",
    "\n",
    "The clusters amoung my test set are similar to my training set. Though they did not model the author, the text groupings they made were meaningful and consistent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
